import streamlit as st
import pandas as pd
import altair as alt
import io
import numpy as np # For statistical calculations like rolling std, correlations
from st_aggrid import AgGrid, GridOptionsBuilder, GridUpdateMode, DataReturnMode, JsCode

# --- Configuration (UPDATE THESE BASED ON YOUR DATA) ---
# IMPORTANT: Replace with your actual sheet names from the Excel workbook
CURRENT_DAY_SHEET_NAME = "DVaR_COB" # DVaR Current COB data
PREVIOUS_DAY_SHEET_NAME = "DVaR_Prev_COB" # DVaR Previous COB data
SVAR_COB_SHEET_NAME = "SVaR_COB" # SVaR Current COB data
SVAR_PREV_COB_SHEET_NAME = "SVaR_Prev_COB" # SVaR Previous COB data

# Node value for DVaR calculations for each asset class
# CONFIRM THESE VALUES!
FX_DVAR_NODE = 10
RATES_DVAR_NODE = 10
EM_MACRO_DVAR_NODE = 10

# PnL Vector range (assuming 260 days from 261 to 520)
PNL_VECTOR_START = 261
PNL_VECTOR_END = 520 # pnl_vector520 is inclusive, covers 260 days

# --- Streamlit Application Setup ---
st.set_page_config(layout="wide", page_title="Market Risk DVaR Tail Analysis")

st.title("üìä Comprehensive Market Risk DVaR Tail Analysis")

st.write(
    """
    Upload your Excel workbook (`Tail_analysis_auto.xlsx`) to perform detailed DVaR and SVaR tail analysis
    for FX, Rates, and EM Macro. Explore trends, contributions, backtesting, and more across your current and previous day data.
    """
)

# --- Debug Toggle ---
DEBUG_MODE = st.checkbox("Show Debug Info (for troubleshooting data loading and processing)", value=False)


# --- Helper Functions ---

@st.cache_data # Cache the data loading to avoid re-running on every interaction
def load_data(file_buffer, debug_mode):
    """
    Loads data from the specified sheets in the Excel workbook.
    Handles date extraction from the first row and sets proper headers.
    Ensures 'Node' column is numeric.
    Includes debug output.
    """
    data_frames = {}
    date_mappings = {}

    # List of all sheets to load
    sheets_to_load = [
        CURRENT_DAY_SHEET_NAME,
        PREVIOUS_DAY_SHEET_NAME,
        SVAR_COB_SHEET_NAME,
        SVAR_PREV_COB_SHEET_NAME
    ]

    for sheet_name in sheets_to_load:
        if debug_mode:
            st.subheader(f"DEBUG: Loading Sheet: {sheet_name}")
        try:
            # IMPORTANT: Reset file_buffer position for each sheet read
            file_buffer.seek(0) 

            # Read the first two rows to get dates and column names
            raw_df_header_dates = pd.read_excel(file_buffer, sheet_name=sheet_name, header=None, nrows=2)
            
            if debug_mode:
                st.write(f"DEBUG: Raw first two rows of '{sheet_name}':")
                st.dataframe(raw_df_header_dates)

            # The first row contains dates, the second row contains actual column names
            dates_row = raw_df_header_dates.iloc[0]
            column_names_row = raw_df_header_dates.iloc[1]

            if debug_mode:
                st.write(f"DEBUG: Dates Row from '{sheet_name}':")
                st.write(dates_row)
                st.write(f"DEBUG: Column Names Row from '{sheet_name}':")
                st.write(column_names_row)


            # Read the actual data, skipping the first two rows
            file_buffer.seek(0) # Reset buffer again before reading full data
            df = pd.read_excel(file_buffer, sheet_name=sheet_name, header=None, skiprows=2)
            df.columns = column_names_row # Assign the second row as column headers
            
            # Drop columns that are entirely NaN after header adjustments (e.g., empty columns in Excel)
            df = df.dropna(axis=1, how='all')

            if debug_mode:
                st.write(f"DEBUG: DataFrame '{sheet_name}' after initial load and column assignment (head):")
                st.dataframe(df.head())
                st.write(f"DEBUG: DataFrame '{sheet_name}' columns:")
                st.write(df.columns.tolist())


            # Ensure 'Node' column is numeric for consistent filtering
            if 'Node' in df.columns:
                df['Node'] = pd.to_numeric(df['Node'], errors='coerce')
                df['Node'] = df['Node'].astype('Int64') # Using nullable integer type
                if debug_mode:
                    st.write(f"DEBUG: 'Node' column dtypes for '{sheet_name}': {df['Node'].dtype}")
            else:
                if debug_mode:
                    st.warning(f"DEBUG: 'Node' column not found in sheet '{sheet_name}'. This might cause issues.")


            # Create mapping for pnl_vector columns to their dates
            pnl_date_map = {}
            for col_idx, col_name in enumerate(column_names_row):
                if pd.isna(col_name): # Skip NaN column names
                    continue
                # Handle both pnl_vectorXXX and pnl_vector[T-2] formats
                if str(col_name).startswith('pnl_vector') or ('[T-2]' in str(col_name) and 'pnl_vector' in str(col_name)): 
                    # Ensure col_idx is within bounds of dates_row
                    if col_idx < len(dates_row):
                        date_val = dates_row.iloc[col_idx]
                        # Convert date_val to datetime, handle potential Excel float dates
                        if isinstance(date_val, (int, float)):
                            try:
                                # Convert Excel float date to datetime
                                # Origin is 1899-12-30 for Excel dates (number of days since 1899-12-30)
                                pnl_date_map[str(col_name)] = pd.to_datetime(date_val, unit='D', origin='1899-12-30')
                            except:
                                pnl_date_map[str(col_name)] = pd.NaT # Not a Time, if conversion fails
                        else:
                            pnl_date_map[str(col_name)] = pd.to_datetime(date_val, errors='coerce') # Coerce errors to NaT
                    else:
                        pnl_date_map[str(col_name)] = pd.NaT # No date found for this column
            
            if debug_mode:
                st.write(f"DEBUG: PnL Date Map for '{sheet_name}':")
                st.write(pnl_date_map)

            data_frames[sheet_name] = df
            date_mappings[sheet_name] = pnl_date_map

        except Exception as e:
            st.error(f"Error loading data from sheet '{sheet_name}': {e}. "
                     "Please ensure the sheet names are correct and the first two rows contain dates/headers as expected.")
            return None, None
    
    return data_frames, date_mappings


def calculate_var_tails(df, pnl_date_map, sheet_type="current", var_type_filter="DVaR", debug_mode=False):
    """
    Calculates VaR tails (DVaR or SVaR) for FX, Rates, EM Macro, and Macro.
    Adds a 'Sheet_Type' column for differentiation.
    Includes debug output.
    """
    if debug_mode:
        st.subheader(f"DEBUG: Calculating VaR Tails for {sheet_type} ({var_type_filter})")
        st.write("DEBUG: Input DataFrame Head:")
        st.dataframe(df.head())
        st.write("DEBUG: Input DataFrame Columns:")
        st.write(df.columns.tolist())
        st.write("DEBUG: Input PnL Date Map:")
        st.write(pnl_date_map)

    # Define common identifying variables
    id_vars = ['Var Type', 'Node', 'Asset class', 'currency', 'sensitivity_type', 'load_code']
    
    # Identify all PnL vector columns that are actual columns in the dataframe
    pnl_vector_cols = [col for col in df.columns if str(col).startswith('pnl_vector') or ('[T-2]' in str(col) and 'pnl_vector' in str(col))]

    valid_pnl_cols = []
    for col in pnl_vector_cols:
        col_str = str(col)
        # DVaR Current COB (pnl_vector261 to pnl_vector520, no [T-2])
        if sheet_type == "current" and var_type_filter == "DVaR":
            if col_str.startswith('pnl_vector') and not '[T-2]' in col_str:
                try:
                    num_part = col_str.replace('pnl_vector', '')
                    if num_part.isdigit(): # Ensure it's purely a number
                        num = int(num_part)
                        if PNL_VECTOR_START <= num <= PNL_VECTOR_END:
                            valid_pnl_cols.append(col)
                except ValueError:
                    continue # Skip if not a valid number after stripping prefix
        # DVaR Previous COB (pnl_vectorXXX[T-2])
        elif sheet_type == "previous" and var_type_filter == "DVaR":
            if col_str.startswith('pnl_vector') and '[T-2]' in col_str:
                valid_pnl_cols.append(col) # All columns with [T-2] suffix
        # SVaR Current COB (pnl_vector1, pnl_vector2, etc., no [T-2])
        elif sheet_type == "current" and var_type_filter == "SVaR":
            if col_str.startswith('pnl_vector') and not '[T-2]' in col_str:
                valid_pnl_cols.append(col) # All pnl_vector columns (e.g., 1, 2, 3...)
        # SVaR Previous COB (pnl_vector1[T-2], etc.)
        elif sheet_type == "previous" and var_type_filter == "SVaR":
            if col_str.startswith('pnl_vector') and '[T-2]' in col_str:
                valid_pnl_cols.append(col) # All pnl_vector[T-2] columns
    
    if debug_mode:
        st.write(f"DEBUG: Detected PnL Vector Columns: {pnl_vector_cols}")
        st.write(f"DEBUG: Valid PnL Columns for current config ({sheet_type}, {var_type_filter}): {valid_pnl_cols}")


    # Ensure all id_vars are present in the dataframe columns before melting
    id_vars_present = [col for col in id_vars if col in df.columns]
    
    # Melt only if valid pnl columns exist
    if not valid_pnl_cols:
        st.warning(f"No valid PnL vector columns found for {sheet_type} ({var_type_filter}) data. Skipping VaR calculation.")
        # Return empty DataFrames instead of None for easier subsequent handling
        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()

    df_melted = df.melt(id_vars=id_vars_present,
                         value_vars=valid_pnl_cols,
                         var_name='Pnl_Vector_Name',
                         value_name='Value')
    
    # --- Add Pnl_Vector_Rank ---
    # Improved logic to handle [T-2] suffix for rank extraction
    def extract_pnl_rank(pnl_vector_name):
        # Remove '[T-2]' suffix if present, then extract digits
        name_without_suffix = pnl_vector_name.split('[T-2]')[0]
        numeric_part = ''.join(filter(str.isdigit, name_without_suffix))
        return int(numeric_part) if numeric_part else np.nan
    
    df_melted['Pnl_Vector_Rank'] = df_melted['Pnl_Vector_Name'].apply(extract_pnl_rank)
    df_melted['Pnl_Vector_Rank'] = df_melted['Pnl_Vector_Rank'].astype('Int64') # Nullable integer

    if debug_mode:
        st.write("DEBUG: DataFrame after melting (head):")
        st.dataframe(df_melted.head())
        st.write("DEBUG: DataFrame after melting (shape):", df_melted.shape)
        st.write("DEBUG: Pnl_Vector_Rank values (sample):", df_melted['Pnl_Vector_Rank'].dropna().unique())
    
    # Add a 'Date' column using the pnl_date_map
    df_melted['Date'] = df_melted['Pnl_Vector_Name'].map(pnl_date_map)
    df_melted['Date'] = pd.to_datetime(df_melted['Date'], errors='coerce') # Coerce errors for NaT dates
    
    # Filter out rows where Date is NaT (if date mapping failed for some columns)
    df_melted = df_melted.dropna(subset=['Date'])
    
    if debug_mode:
        st.write("DEBUG: DataFrame after Date mapping and dropping NaT Dates (head):")
        st.dataframe(df_melted.head())
        st.write("DEBUG: DataFrame after Date mapping and dropping NaT Dates (shape):", df_melted.shape)


    # Filter for the specific Var Type (DVaR or SVaR) as the sheets are now distinct
    df_filtered_var_type = df_melted[df_melted['Var Type'] == var_type_filter].copy()
    
    if debug_mode:
        st.write(f"DEBUG: DataFrame after filtering for 'Var Type' == '{var_type_filter}' (head):")
        st.dataframe(df_filtered_var_type.head())
        st.write(f"DEBUG: DataFrame after filtering for 'Var Type' == '{var_type_filter}' (shape):", df_filtered_var_type.shape)

    # --- Calculate VaR for each asset class ---
    asset_classes = ['fx', 'rates', 'em macro']
    asset_var_dfs = {}
    
    node_mapping = {
        'fx': FX_DVAR_NODE, # Using DVaR nodes for SVaR as well, confirm if different needed
        'rates': RATES_DVAR_NODE,
        'em macro': EM_MACRO_DVAR_NODE
    }

    for ac in asset_classes:
        # Ensure comparison with nullable integer type for Node
        node_val_for_filter = node_mapping.get(ac, FX_DVAR_NODE)
        filtered_df = df_filtered_var_type[
            (df_filtered_var_type['Asset class'] == ac) & # Keep Asset class filter
            (df_filtered_var_type['Node'] == node_val_for_filter)
        ]
        if debug_mode:
            st.write(f"DEBUG: Filtered data for Asset Class '{ac}' and Node '{node_val_for_filter}' (head):")
            st.dataframe(filtered_df.head())
            st.write(f"DEBUG: Filtered data for Asset Class '{ac}' and Node '{node_val_for_filter}' (shape):", filtered_df.shape)

        if not filtered_df.empty:
            asset_var_dfs[ac] = filtered_df.groupby(['Date', 'Pnl_Vector_Name', 'Pnl_Vector_Rank'])['Value'].sum().reset_index(name=f'{ac.replace(" ", "_")}_{var_type_filter}_Value')
            asset_var_dfs[ac]['Sheet_Type'] = sheet_type # Add sheet type for merging
        else:
            # Ensure an empty DataFrame with expected columns if no data found
            asset_var_dfs[ac] = pd.DataFrame(columns=['Date', 'Pnl_Vector_Name', 'Pnl_Vector_Rank', f'{ac.replace(" ", "_")}_{var_type_filter}_Value', 'Sheet_Type'])
            
    # --- Combine all VaR tails for Macro VaR ---
    macro_var_df = pd.DataFrame(columns=['Date', 'Pnl_Vector_Name', 'Pnl_Vector_Rank', f'Macro_{var_type_filter}_Value', 'Sheet_Type']) # Initialize empty DF
    
    if 'fx' in asset_var_dfs and not asset_var_dfs['fx'].empty:
        macro_var_df = asset_var_dfs['fx']
        for ac in ['rates', 'em macro']:
            if ac in asset_var_dfs and not asset_var_dfs[ac].empty:
                # Merge with Pnl_Vector_Rank as a key
                macro_var_df = pd.merge(macro_var_df, asset_var_dfs[ac], on=['Date', 'Pnl_Vector_Name', 'Pnl_Vector_Rank', 'Sheet_Type'], how='outer')
        
        # Calculate Macro VaR
        macro_var_df[f'Macro_{var_type_filter}_Value'] = macro_var_df[[f'{ac.replace(" ", "_")}_{var_type_filter}_Value' for ac in asset_classes]].sum(axis=1)
        macro_var_df = macro_var_df.sort_values('Date').reset_index(drop=True)
        macro_var_df['Sheet_Type'] = sheet_type # Ensure this column is consistent
    
    if debug_mode:
        st.write(f"DEBUG: Final Macro DVaR DF ({sheet_type}, {var_type_filter}) head and shape:")
        st.dataframe(macro_var_df.head())
        st.write(macro_var_df.shape)

    # Return individual asset class VaR DFs, macro VaR DF, and the raw filtered VaR DF for sensitivity analysis
    return asset_var_dfs.get('fx', pd.DataFrame()), \
           asset_var_dfs.get('rates', pd.DataFrame()), \
           asset_var_dfs.get('em macro', pd.DataFrame()), \
           macro_var_df, \
           df_filtered_var_type # This is the raw data filtered by Var Type for sensitivity analysis

# --- Visualization Functions ---

def plot_dvar_trends(df, title, y_column, legend_title="Type"):
    """Generates a line chart for DVaR trends."""
    if df.empty: return # Added guard
    chart = alt.Chart(df).mark_line(point=True).encode(
        x=alt.X('Date:T', title='Date', axis=alt.Axis(format='%d-%m-%Y')), # Date format change
        y=alt.Y(y_column, title='DVaR Value'),
        color=alt.Color('Sheet_Type:N', title=legend_title),
        tooltip=[alt.Tooltip('Date:T', format='%d-%m-%Y'), y_column, 'Sheet_Type', 'Pnl_Vector_Rank'] # Date format change, added rank
    ).properties(
        title=title
    ).interactive() # Enable zooming and panning
    st.altair_chart(chart, use_container_width=True)

def plot_dvar_volatility(df, title):
    """Generates a line chart for rolling DVaR volatility."""
    if df.empty or 'Rolling_Std_DVaR' not in df.columns: # Added guard
        st.warning("Rolling Standard Deviation column not found or DataFrame is empty for volatility plot.")
        return

    chart = alt.Chart(df).mark_line(point=True).encode(
        x=alt.X('Date:T', title='Date', axis=alt.Axis(format='%d-%m-%Y')), # Date format change
        y=alt.Y('Rolling_Std_DVaR', title='Rolling Std Dev of DVaR'),
        color=alt.Color('Sheet_Type:N', title='Data Type'),
        tooltip=[alt.Tooltip('Date:T', format='%d-%m-%Y'), 'Rolling_Std_DVaR', 'Sheet_Type'] # Date format change
    ).properties(
        title=title
    ).interactive()
    st.altair_chart(chart, use_container_width=True)

def plot_contribution(df, title):
    """Generates a stacked area chart for asset class DVaR contributions."""
    if df.empty: 
        st.info(f"No data available for {title} chart.")
        return # Added guard
    df_long = df.melt(id_vars=['Date', 'Pnl_Vector_Name', 'Macro_DVaR_Value', 'Sheet_Type'],
                      value_vars=[col for col in df.columns if '_DVaR_Value' in col and col != 'Macro_DVaR_Value'],
                      var_name='Asset_Class',
                      value_name='Contribution_Value')
    
    # Ensure Macro_DVaR_Value is not zero to avoid division by zero
    # Handle cases where Total_DVaR is zero, set contribution to 0 if individual contribution is also 0.
    df_long['Contribution_Percentage'] = np.where(
        (df_long['Macro_DVaR_Value'] != 0),
        (df_long['Contribution_Value'] / df_long['Macro_DVaR_Value']) * 100,
        np.where(df_long['Contribution_Value'] == 0, 0, np.nan) # If Macro_DVaR_Value is 0, and Contribution_Value is also 0, then 0. Else NaN.
    )
    
    df_long = df_long.dropna(subset=['Contribution_Percentage'])

    if df_long.empty: # Added guard after dropna
        st.info(f"No valid contribution data after filtering for {title}.")
        return

    chart = alt.Chart(df_long).mark_area().encode(
        x=alt.X('Date:T', title='Date', axis=alt.Axis(format='%d-%m-%Y')), # Date format change
        y=alt.Y('Contribution_Percentage', title='Percentage Contribution (%)', stack='normalize'),
        color=alt.Color('Asset_Class:N', title='Asset Class'),
        tooltip=[alt.Tooltip('Date:T', format='%d-%m-%Y'), 'Asset_Class:N', alt.Tooltip('Contribution_Percentage', format='.2f%')] # CORRECTED LINE
    ).properties(
        title=title
    ).interactive()
    st.altair_chart(chart, use_container_width=True)

def display_correlations(df):
    """Calculates and displays correlation matrix for DVaR asset classes."""
    if df.empty:
        st.info("No data available to calculate correlations.")
        return

    # Select only the DVaR value columns for current day data for correlation
    df_current_day = df[df['Sheet_Type'] == 'current']
    if df_current_day.empty:
        st.info("No 'Current Day' data available for correlation calculation.")
        return

    # Filter for columns that contain the DVaR values for correlation
    dvar_value_cols = [col for col in df_current_day.columns if '_DVaR_Value' in col and col != 'Macro_DVaR_Value']
    
    if not dvar_value_cols:
        st.info("No individual asset class DVaR values found for correlation calculation.")
        return

    correlation_df = df_current_day[dvar_value_cols].corr()
    st.subheader("DVaR Asset Class Correlations (Current Day)")
    st.dataframe(correlation_df.style.background_gradient(cmap='viridis', axis=None).format("{:.2f}"))
    st.markdown("Higher positive values indicate stronger positive correlation, negative values indicate inverse correlation.")


def plot_exceedances(df, title, threshold):
    """Plots DVaR values and highlights exceedances."""
    if threshold is None or df.empty:
        st.warning("Please set a threshold and ensure data is available for exceedance analysis.")
        return

    df_plot = df.copy() # Work on a copy to avoid SettingWithCopyWarning
    df_plot['Exceeds_Threshold'] = df_plot['Macro_DVaR_Value'] > threshold

    chart = alt.Chart(df_plot).mark_line().encode(
        x=alt.X('Date:T', title='Date', axis=alt.Axis(format='%d-%m-%Y')), # Date format change
        y=alt.Y('Macro_DVaR_Value', title='Macro DVaR Value'),
        tooltip=[alt.Tooltip('Date:T', format='%d-%m-%Y'), 'Macro_DVaR_Value'] # Date format change
    ).properties(
        title=title
    )

    exceedance_points = alt.Chart(df_plot[df_plot['Exceeds_Threshold']]).mark_point(color='red', size=100).encode(
        x=alt.X('Date:T'),
        y=alt.Y('Macro_DVaR_Value'),
        tooltip=[alt.Tooltip('Date:T', format='%d-%m-%Y'), 'Macro_DVaR_Value', {'title': 'Status', 'value': 'Exceeded Threshold'}] # CORRECTED LINE
    )

    threshold_line = alt.Chart(pd.DataFrame({'threshold': [threshold]})).mark_rule(color='red', strokeDash=[5,5]).encode(
        y='threshold'
    )

    st.altair_chart(chart + exceedance_points + threshold_line, use_container_width=True)
    st.info(f"Number of times Macro DVaR exceeded {threshold:.2f}: {df_plot['Exceeds_Threshold'].sum()}")


def plot_svar_dvar_comparison(dvar_df, svar_df, title):
    """Compares Macro DVaR and Macro SVaR."""
    if svar_df.empty or dvar_df.empty:
        st.info("SVaR or DVaR data not available for comparison.")
        return
    
    # Ensure we are comparing 'current' vs 'current' or 'previous' vs 'previous'
    # Merge DVaR and SVaR on Date and Pnl_Vector_Name
    comparison_df = pd.merge(dvar_df[['Date', 'Pnl_Vector_Name', 'Macro_DVaR_Value', 'Sheet_Type']],
                             svar_df[['Date', 'Pnl_Vector_Name', 'Macro_SVaR_Value', 'Sheet_Type']],
                             on=['Date', 'Pnl_Vector_Name', 'Sheet_Type'], how='inner')
    
    if comparison_df.empty:
        st.info("No common dates/pnl vectors found between DVaR and SVaR data for comparison. "
                "Ensure both sheets have corresponding entries and dates.")
        return

    # Melt for Altair
    df_melted = comparison_df.melt(id_vars=['Date', 'Pnl_Vector_Name', 'Sheet_Type'],
                                   value_vars=['Macro_DVaR_Value', 'Macro_SVaR_Value'],
                                   var_name='VaR_Type',
                                   value_name='Value')

    chart = alt.Chart(df_melted).mark_line(point=True).encode(
        x=alt.X('Date:T', title='Date', axis=alt.Axis(format='%d-%m-%Y')), # Date format change
        y=alt.Y('Value', title='VaR Value'),
        color=alt.Color('VaR_Type:N', title='VaR Type'),
        strokeDash='VaR_Type:N', # Different line styles for clarity
        tooltip=[alt.Tooltip('Date:T', format='%d-%m-%Y'), 'VaR_Type', 'Value', 'Sheet_Type'] # Date format change
    ).properties(
        title=title
    ).interactive()
    st.altair_chart(chart, use_container_width=True)


def plot_sensitivity_attribution(df_raw_var_type, pnl_date_map, selected_asset_class):
    """
    Calculates and plots VaR contribution by sensitivity type for a selected asset class.
    df_raw_var_type is the full raw df for the specific VAR type (DVaR or SVaR).
    """
    if df_raw_var_type.empty:
        st.info("Raw VaR data not available for sensitivity attribution.")
        return

    # Dynamically get the node value for the selected asset class
    node_config_key = f"{selected_asset_class.replace(' ', '_').upper()}_DVAR_NODE"
    node_value_for_filter = globals().get(node_config_key)
    
    if node_value_for_filter is None:
        st.error(f"Configuration error: Node value for '{selected_asset_class}' (expected key: {node_config_key}) is not defined in the script's configuration section. Please check the `FX_DVAR_NODE`, `RATES_DVAR_NODE`, `EM_MACRO_DVAR_NODE` definitions.")
        return

    # Filter data for the selected asset class and node
    df_filtered = df_raw_var_type[
        (df_raw_var_type['Asset class'] == selected_asset_class) &
        (df_raw_var_type['Node'] == node_value_for_filter)
    ].copy()
    
    if df_filtered.empty:
        # Changed f-string to explicitly convert node_value_for_filter to string
        st.info(f"No DVaR data found for '{selected_asset_class}' with Node {str(node_value_for_filter)}. "
                "Please ensure this combination exists in your data and the node configuration is correct.")
        return

    # Group by Date, Pnl_Vector_Name, and Sensitivity Type to sum values
    sensitivity_contributions = df_filtered.groupby(['Date', 'Pnl_Vector_Name', 'sensitivity_type'])['Value'].sum().reset_index()
    
    # Calculate total DVaR for normalization
    total_dvar_by_date = sensitivity_contributions.groupby(['Date', 'Pnl_Vector_Name'])['Value'].sum().reset_index(name='Total_DVaR')
    
    # Merge total DVaR back
    sensitivity_contributions = pd.merge(sensitivity_contributions, total_dvar_by_date, on=['Date', 'Pnl_Vector_Name'], how='left')
    
    # Calculate percentage contribution
    # Handle cases where Total_DVaR is zero: set contribution to 0 if individual contribution is also 0, else NaN.
    sensitivity_contributions['Percentage_Contribution'] = np.where(
        (sensitivity_contributions['Total_DVaR'] != 0),
        (sensitivity_contributions['Value'] / sensitivity_contributions['Total_DVaR']) * 100,
        np.where(sensitivity_contributions['Value'] == 0, 0, np.nan)
    )
    
    sensitivity_contributions.replace([np.inf, -np.inf], np.nan, inplace=True)
    sensitivity_contributions.dropna(subset=['Percentage_Contribution'], inplace=True)

    if sensitivity_contributions.empty:
        st.info(f"No valid sensitivity contribution data after filtering for {selected_asset_class}.")
        return

    # Sort for consistent stacking
    sensitivity_contributions = sensitivity_contributions.sort_values(['Date', 'Pnl_Vector_Name', 'Percentage_Contribution'], ascending=[True, True, False])

    # Get top N sensitivities for display, group others
    top_n = st.slider(f"Show Top N Sensitivities for {selected_asset_class}", 5, 20, 10, key=f"top_n_sens_{selected_asset_class}")
    
    # Calculate average contribution of each sensitivity to identify top N
    # Use 'Value' for ranking as percentage can be misleading if total is very small
    avg_sens_contrib = sensitivity_contributions.groupby('sensitivity_type')['Value'].mean().nlargest(top_n).index
    
    sensitivity_contributions['Display_Sensitivity'] = sensitivity_contributions['sensitivity_type'].apply(
        lambda x: x if x in avg_sens_contrib else 'Other'
    )
    
    # Recalculate values and contributions for 'Other' category if it exists
    sensitivity_contributions_display = sensitivity_contributions.groupby(['Date', 'Pnl_Vector_Name', 'Display_Sensitivity'])['Value'].sum().reset_index()
    
    total_dvar_by_date_recalculated = sensitivity_contributions_display.groupby(['Date', 'Pnl_Vector_Name'])['Value'].sum().reset_index(name='Total_DVaR')
    sensitivity_contributions_display = pd.merge(sensitivity_contributions_display, total_dvar_by_date_recalculated, on=['Date', 'Pnl_Vector_Name'], how='left')
    
    sensitivity_contributions_display['Percentage_Contribution'] = (sensitivity_contributions_display['Value'] / sensitivity_contributions_display['Total_DVaR']) * 100
    
    sensitivity_contributions_display.replace([np.inf, -np.inf], np.nan, inplace=True)
    sensitivity_contributions_display.dropna(subset=['Percentage_Contribution'], inplace=True)

    if sensitivity_contributions_display.empty:
        st.info(f"No displayable sensitivity contribution data after grouping for {selected_asset_class}.")
        return

    # Ensure Date is datetime for Altair
    sensitivity_contributions_display['Date'] = pd.to_datetime(sensitivity_contributions_display['Date'])

    chart = alt.Chart(sensitivity_contributions_display).mark_area().encode(
        x=alt.X('Date:T', title='Date', axis=alt.Axis(format='%d-%m-%Y')), # Date format change
        y=alt.Y('Percentage_Contribution', title='Percentage Contribution (%)', stack='normalize'),
        color=alt.Color('Display_Sensitivity:N', title='Sensitivity Type', legend=alt.Legend(columns=2)),
        order=alt.Order('Percentage_Contribution', sort='descending'),
        tooltip=[alt.Tooltip('Date:T', format='%d-%m-%Y'), 'Display_Sensitivity:N', alt.Tooltip('Percentage_Contribution', format='.2f%'), 'Value'] # Date format change
    ).properties(
        title=f"DVaR Contribution by Sensitivity Type for {selected_asset_class}"
    ).interactive()
    st.altair_chart(chart, use_container_width=True)

def display_top_bottom_tails_table(macro_dvar_curr, macro_dvar_prev, fx_dvar_curr, fx_dvar_prev, rates_dvar_curr, rates_dvar_prev, em_macro_dvar_curr, em_macro_dvar_prev, debug_mode):
    """
    Generates and displays the top 20 positive and negative Macro DVaR tails table using AgGrid.
    """
    st.header("üèÜ Top/Bottom DVaR Tails Analysis")
    st.markdown("Identify the top 20 positive and negative Macro DVaR tails and their corresponding asset class contributions, showing change from previous COB.")

    # List of all dataframes and their intended current/previous prefixes for merging
    # Ensure correct keys for these dataframes are used. They should have 'Date', 'Pnl_Vector_Name', 'Pnl_Vector_Rank', and the specific DVaR_Value
    data_sources = [
        (macro_dvar_curr, macro_dvar_prev, 'Macro'),
        (fx_dvar_curr, fx_dvar_prev, 'FX'),
        (rates_dvar_curr, rates_dvar_prev, 'Rates'),
        (em_macro_dvar_curr, em_macro_dvar_prev, 'EM_Macro')
    ]

    # Common merge keys
    common_merge_keys = ['Date', 'Pnl_Vector_Name', 'Pnl_Vector_Rank']

    # Prepare individual combined (current+previous) DFs for each asset class
    combined_asset_dfs_for_final_merge = []

    for current_df, previous_df, prefix in data_sources:
        current_val_col = f'{prefix}_DVaR_Value'
        previous_val_col = f'{prefix}_DVaR_Value' # The actual column name in the _prev DFs before suffixing

        # Ensure DataFrames have the required columns before subsetting
        curr_df_for_merge = pd.DataFrame()
        if not current_df.empty and current_val_col in current_df.columns:
            curr_df_for_merge = current_df[common_merge_keys + [current_val_col]].copy()
            curr_df_for_merge.rename(columns={current_val_col: f'{prefix}_DVaR_Value_Current'}, inplace=True)
        elif debug_mode:
            st.write(f"DEBUG: Skipping current data for {prefix} - DataFrame empty or missing {current_val_col}.")

        prev_df_for_merge = pd.DataFrame()
        if not previous_df.empty and previous_val_col in previous_df.columns:
            prev_df_for_merge = previous_df[common_merge_keys + [previous_val_col]].copy()
            prev_df_for_merge.rename(columns={previous_val_col: f'{prefix}_DVaR_Value_Previous'}, inplace=True)
        elif debug_mode:
            st.write(f"DEBUG: Skipping previous data for {prefix} - DataFrame empty or missing {previous_val_col}.")
        
        # Outer merge current and previous for this asset class
        if not curr_df_for_merge.empty or not prev_df_for_merge.empty:
            merged_for_asset = pd.merge(curr_df_for_merge, prev_df_for_merge, 
                                        on=common_merge_keys, how='outer')
            combined_asset_dfs_for_final_merge.append(merged_for_asset)
            if debug_mode:
                st.write(f"DEBUG: Combined DF for {prefix} (head):")
                st.dataframe(merged_for_asset.head())
                st.write(f"DEBUG: Combined DF for {prefix} (columns):", merged_for_asset.columns.tolist())
        elif debug_mode:
            st.write(f"DEBUG: No data to merge for {prefix} asset class.")

    if not combined_asset_dfs_for_final_merge:
        st.info("No DVaR data available for current or previous COB after initial processing. Cannot generate Top/Bottom Tails table.")
        return

    # Perform the final merge of all combined asset DFs
    all_assets_combined = combined_asset_dfs_for_final_merge[0]
    for i in range(1, len(combined_asset_dfs_for_final_merge)):
        all_assets_combined = pd.merge(all_assets_combined, combined_asset_dfs_for_final_merge[i], on=common_merge_keys, how='outer')

    if debug_mode:
        st.write("DEBUG: all_assets_combined DataFrame before final fillna and change calculation (head):")
        st.dataframe(all_assets_combined.head())
        st.write("DEBUG: all_assets_combined DataFrame columns before final fillna:", all_assets_combined.columns.tolist())

    # Fill NaN values with 0 for all DVaR value columns
    # Find all columns that contain 'DVaR_Value' (current or previous)
    dvar_value_columns_in_final_df = [col for col in all_assets_combined.columns if '_DVaR_Value_Current' in col or '_DVaR_Value_Previous' in col]
    all_assets_combined[dvar_value_columns_in_final_df] = all_assets_combined[dvar_value_columns_in_final_df].fillna(0)

    # Calculate Change columns
    for prefix in ['Macro', 'FX', 'Rates', 'EM_Macro']:
        current_col = f'{prefix}_DVaR_Value_Current'
        previous_col = f'{prefix}_DVaR_Value_Previous'
        change_col = f'{prefix}_DVaR_Change'
        
        # Ensure both current and previous columns exist after fillna(0) for direct subtraction
        if current_col in all_assets_combined.columns and previous_col in all_assets_combined.columns:
            all_assets_combined[change_col] = all_assets_combined[current_col] - all_assets_combined[previous_col]
        else:
            all_assets_combined[change_col] = 0 # Default to 0 if expected columns are genuinely missing


    # Drop rows where Macro_DVaR_Value_Current is still NaN (should be covered by fillna(0) but safety check)
    all_assets_combined.dropna(subset=['Macro_DVaR_Value_Current'], inplace=True)
    
    # Sort by Date and Pnl_Vector_Rank for consistent ordering
    all_assets_combined = all_assets_combined.sort_values(by=['Date', 'Pnl_Vector_Rank']).reset_index(drop=True)

    if debug_mode:
        st.write("DEBUG: Final all_assets_combined DataFrame after all merges, fillna, and change calculation (head):")
        st.dataframe(all_assets_combined.head())
        st.write("DEBUG: Final all_assets_combined DataFrame columns:", all_assets_combined.columns.tolist())
        st.write("DEBUG: Sample of Macro_DVaR_Value_Current values (after all processing):", all_assets_combined['Macro_DVaR_Value_Current'].head())


    # Identify Top 20 Positive and Negative Macro Tails
    if all_assets_combined.empty:
        st.info("No sufficient DVaR data to identify top/bottom tails after all processing steps.")
        return

    top_20_positive = all_assets_combined.nlargest(min(20, len(all_assets_combined)), 'Macro_DVaR_Value_Current', keep='all')
    top_20_negative = all_assets_combined.nsmallest(min(20, len(all_assets_combined)), 'Macro_DVaR_Value_Current', keep='all')

    # Define AgGrid column definitions and cell styles
    columnDefs = [
        {"field": "Date", "headerName": "Date", "type": ["dateColumnFilter", "customDateTimeFormat"], "custom_format_string": 'dd-MM-yyyy'},
        {"field": "Pnl_Vector_Name", "headerName": "P&L Vector"},
    ]

    # JavaScript code for cell styling (red for negative, green for positive)
    change_cell_style_jscode = JsCode("""
    function(params) {
        if (params.value < 0) {
            return {backgroundColor: '#ffe6e6', color: 'red'}; // Light red background, red text
        } else if (params.value > 0) {
            return {backgroundColor: '#e6ffe6', color: 'green'}; // Light green background, green text
        }
        return null;
    }
    """)
    
    # Format number with thousands separator and 2 decimal places
    number_formatter_jscode = JsCode("function(params) { return params.value.toLocaleString(undefined, {minimumFractionDigits: 2, maximumFractionDigits: 2}); }")

    asset_prefixes = ['Macro', 'FX', 'Rates', 'EM_Macro']
    for prefix in asset_prefixes:
        columnDefs.append({"field": f"{prefix}_DVaR_Value_Current", "headerName": f"{prefix} Current", "type": ["numericColumn", "numberColumnFilter"], "valueFormatter": number_formatter_jscode})
        columnDefs.append({"field": f"{prefix}_DVaR_Value_Previous", "headerName": f"{prefix} Previous", "type": ["numericColumn", "numberColumnFilter"], "valueFormatter": number_formatter_jscode})
        columnDefs.append({"field": f"{prefix}_DVaR_Change", "headerName": f"{prefix} Change", "type": ["numericColumn", "numberColumnFilter"], "cellStyle": change_cell_style_jscode, "valueFormatter": number_formatter_jscode})

    # Build GridOptions
    gb = GridOptionsBuilder.from_dataframe(pd.DataFrame(columns=[col['field'] for col in columnDefs])) # Dummy DF for builder
    gb.configure_columns(columnDefs)
    gb.configure_grid_options(domLayout='autoHeight') # Adjust grid height automatically
    gridOptions = gb.build()


    st.subheader("Top 20 Positive Macro DVaR Tails (Current COB)")
    if not top_20_positive.empty:
        top_20_positive_display = top_20_positive.copy()
        # Convert Date column to string for AgGrid (YYYY-MM-DD for custom_format_string to work reliably)
        top_20_positive_display['Date'] = top_20_positive_display['Date'].dt.strftime('%Y-%m-%d') 
        AgGrid(top_20_positive_display, gridOptions=gridOptions, 
               data_return_mode='AS_INPUT', update_mode='MODEL_CHANGED', 
               fit_columns_on_grid_load=True, allow_unsafe_jscode=True, 
               enable_enterprise_modules=True, height=350, width='100%', reload_data=True, key='ag_pos_tails')
    else:
        st.info("No positive Macro DVaR tails found.")

    st.subheader("Top 20 Negative Macro DVaR Tails (Current COB)")
    if not top_20_negative.empty:
        top_20_negative_display = top_20_negative.copy()
        # Convert Date column to string for AgGrid (YYYY-MM-DD for custom_format_string to work reliably)
        top_20_negative_display['Date'] = top_20_negative_display['Date'].dt.strftime('%Y-%m-%d') 
        AgGrid(top_20_negative_display, gridOptions=gridOptions, 
               data_return_mode='AS_INPUT', update_mode='MODEL_CHANGED', 
               fit_columns_on_grid_load=True, allow_unsafe_jscode=True, 
               enable_enterprise_modules=True, height=350, width='100%', reload_data=True, key='ag_neg_tails')
    else:
        st.info("No negative Macro DVaR tails found.")


# --- Main Application Logic ---
uploaded_file = st.file_uploader("Choose your Excel file (Tail_analysis_auto.xlsx)", type="xlsx")

if uploaded_file is not None:
    data_sheets, date_mappings = load_data(uploaded_file, DEBUG_MODE) # Pass DEBUG_MODE

    if data_sheets is None or date_mappings is None:
        st.stop() # Stop if loading failed

    # Retrieve all four dataframes and their date mappings
    current_day_df = data_sheets.get(CURRENT_DAY_SHEET_NAME)
    previous_day_df = data_sheets.get(PREVIOUS_DAY_SHEET_NAME)
    svar_cob_df = data_sheets.get(SVAR_COB_SHEET_NAME)
    svar_prev_cob_df = data_sheets.get(SVAR_PREV_COB_SHEET_NAME)

    current_day_date_map = date_mappings.get(CURRENT_DAY_SHEET_NAME)
    previous_day_date_map = date_mappings.get(PREVIOUS_DAY_SHEET_NAME)
    svar_cob_date_map = date_mappings.get(SVAR_COB_SHEET_NAME)
    svar_prev_cob_date_map = date_mappings.get(SVAR_PREV_COB_SHEET_NAME)

    # Basic checks for loaded dataframes
    if not all([current_day_df is not None, previous_day_df is not None, 
                svar_cob_df is not None, svar_prev_cob_df is not None]):
        st.error("One or more required sheets could not be loaded. Please check sheet names and file content.")
        st.stop()
    if not all([current_day_date_map is not None, previous_day_date_map is not None,
                svar_cob_date_map is not None, svar_prev_cob_date_map is not None]):
        st.error("Date mappings could not be extracted for one or more sheets. Check the first row of your Excel sheets.")
        st.stop()

    st.success("Excel data loaded and dates extracted successfully!")

    with st.spinner("Calculating VaR tails... This may take a moment."):
        # Calculate DVaR for Current COB
        fx_dvar_curr, rates_dvar_curr, em_macro_dvar_curr, macro_dvar_curr, raw_dvar_curr = \
            calculate_var_tails(current_day_df, current_day_date_map, "current", "DVaR", DEBUG_MODE) # Pass DEBUG_MODE
        
        # Calculate DVaR for Previous COB
        fx_dvar_prev, rates_dvar_prev, em_macro_dvar_prev, macro_dvar_prev, raw_dvar_prev = \
            calculate_var_tails(previous_day_df, previous_day_date_map, "previous", "DVaR", DEBUG_MODE) # Pass DEBUG_MODE
        
        # Calculate SVaR for Current COB
        fx_svar_curr, rates_svar_curr, em_macro_svar_curr, macro_svar_curr, raw_svar_curr = \
            calculate_var_tails(svar_cob_df, svar_cob_date_map, "current", "SVaR", DEBUG_MODE) # Pass DEBUG_MODE

        # Calculate SVaR for Previous COB
        fx_svar_prev, rates_svar_prev, em_macro_svar_prev, macro_svar_prev, raw_svar_prev = \
            calculate_var_tails(svar_prev_cob_df, svar_prev_cob_date_map, "previous", "SVaR", DEBUG_MODE) # Pass DEBUG_MODE

    if not macro_dvar_curr.empty: # Check if current DVaR macro data exists
        st.success("DVaR and SVaR calculations complete for all sheets!")
        # Display Top/Bottom Tails table immediately
        display_top_bottom_tails_table(macro_dvar_curr, macro_dvar_prev, fx_dvar_curr, fx_dvar_prev, rates_dvar_curr, rates_dvar_prev, em_macro_dvar_curr, em_macro_dvar_prev, DEBUG_MODE)
    else:
        st.warning("No DVaR data could be calculated. Please check your Excel file's format and content and enable debug mode for more details.")
        st.stop() # Stop execution if no DVaR data to display


    # --- Analysis Tabs ---
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "DVaR Trends", "Volatility", "Contribution", "Correlations",
        "Sensitivity Attribution", "SVaR Comparison"
    ])

    with tab1:
        st.header("üìà DVaR Time Series Trends")
        st.markdown("Visualize the evolution of Macro DVaR and individual Asset Class DVaRs over time.")

        # Concatenate DVaR data for "Current vs. Previous Day" trend plots
        all_macro_dvar = pd.concat([macro_dvar_curr, macro_dvar_prev], ignore_index=True) if not macro_dvar_curr.empty and not macro_dvar_prev.empty else (macro_dvar_curr if not macro_dvar_curr.empty else macro_dvar_prev)
        all_fx_dvar = pd.concat([fx_dvar_curr, fx_dvar_prev], ignore_index=True) if not fx_dvar_curr.empty and not fx_dvar_prev.empty else (fx_dvar_curr if not fx_dvar_curr.empty else fx_dvar_prev)
        all_rates_dvar = pd.concat([rates_dvar_curr, rates_dvar_prev], ignore_index=True) if not rates_dvar_curr.empty and not rates_dvar_prev.empty else (rates_dvar_curr if not rates_dvar_curr.empty else rates_dvar_prev)
        all_em_macro_dvar = pd.concat([em_macro_dvar_curr, em_macro_dvar_prev], ignore_index=True) if not em_macro_dvar_curr.empty and not em_macro_dvar_prev.empty else (em_macro_dvar_curr if not em_macro_dvar_curr.empty else em_macro_dvar_prev)


        if not all_macro_dvar.empty:
            plot_dvar_trends(all_macro_dvar, "Macro DVaR Trend (Current vs. Previous Day)", 'Macro_DVaR_Value')
            
            st.subheader("Individual Asset Class DVaR Trends")
            col1, col2, col3 = st.columns(3)
            with col1:
                if not all_fx_dvar.empty:
                    plot_dvar_trends(all_fx_dvar, "FX DVaR Trend", 'FX_DVaR_Value')
            with col2:
                if not all_rates_dvar.empty:
                    plot_dvar_trends(all_rates_dvar, "Rates DVaR Trend", 'Rates_DVaR_Value')
            with col3:
                if not all_em_macro_dvar.empty:
                    plot_dvar_trends(all_em_macro_dvar, "EM Macro DVaR Trend", 'EM_Macro_DVaR_Value')
        else:
            st.info("No DVaR data to display trends.")

    with tab2:
        st.header("üìâ DVaR Volatility Analysis")
        st.markdown("Understand the stability of your DVaR over time by examining rolling standard deviation.")

        window_size = st.slider("Select Rolling Window Size (days)", 5, 60, 20, key='dvar_vol_window')
        
        if not all_macro_dvar.empty:
            # Calculate rolling std for Macro DVaR
            all_macro_dvar['Rolling_Std_DVaR'] = all_macro_dvar.groupby('Sheet_Type')['Macro_DVaR_Value'].transform(lambda x: x.rolling(window=window_size, min_periods=1).std())
            plot_dvar_volatility(all_macro_dvar, f"Macro DVaR Rolling Volatility ({window_size}-day window)")

            st.subheader("Individual Asset Class DVaR Volatility")
            col1, col2, col3 = st.columns(3)
            with col1:
                if not all_fx_dvar.empty:
                    all_fx_dvar['Rolling_Std_DVaR'] = all_fx_dvar.groupby('Sheet_Type')['FX_DVaR_Value'].transform(lambda x: x.rolling(window=window_size, min_periods=1).std())
                    plot_dvar_volatility(all_fx_dvar, f"FX DVaR Rolling Volatility ({window_size}-day window)")
            with col2:
                if not all_rates_dvar.empty:
                    all_rates_dvar['Rolling_Std_DVaR'] = all_rates_dvar.groupby('Sheet_Type')['Rates_DVaR_Value'].transform(lambda x: x.rolling(window=window_size, min_periods=1).std())
                    plot_dvar_volatility(all_rates_dvar, f"Rates DVaR Rolling Volatility ({window_size}-day window)")
            with col3:
                if not all_em_macro_dvar.empty:
                    all_em_macro_dvar['Rolling_Std_DVaR'] = all_em_macro_dvar.groupby('Sheet_Type')['EM_Macro_DVaR_Value'].transform(lambda x: x.rolling(window=window_size, min_periods=1).std())
                    plot_dvar_volatility(all_em_macro_dvar, f"EM Macro DVaR Rolling Volatility ({window_size}-day window)")
        else:
            st.info("No DVaR data to analyze volatility.")

    with tab3:
        st.header("üèõÔ∏è Asset Class Contribution to Macro DVaR")
        st.markdown("See how each asset class contributes to the overall Macro DVaR over time. Only for 'Current Day' DVaR data.")
        if not macro_dvar_curr.empty:
            plot_contribution(macro_dvar_curr, "Asset Class Contribution to Macro DVaR (Current Day)")
        else:
            st.info("No 'Current Day' Macro DVaR data to show contribution.")

    with tab4:
        st.header("ü§ù Asset Class DVaR Correlations")
        st.markdown("Examine the correlation between DVaR values of different asset classes. Only for 'Current Day' DVaR data.")
        if not macro_dvar_curr.empty:
            display_correlations(macro_dvar_curr)
        else:
            st.info("No 'Current Day' DVaR data to calculate correlations.")


    with tab5: # Changed from tab6 to tab5
        st.header("üî¨ DVaR Sensitivity Attribution")
        st.markdown("Break down DVaR by underlying sensitivity types within each asset class. Only for 'Current Day' DVaR data.")
        
        if raw_dvar_curr is not None and not raw_dvar_curr.empty:
            asset_class_options = raw_dvar_curr['Asset class'].unique().tolist()
            # Filter asset classes to ensure only 'fx', 'rates', 'em macro' are options
            relevant_asset_classes = [ac for ac in asset_class_options if ac in ['fx', 'rates', 'em macro']]
            
            if relevant_asset_classes:
                selected_asset_class_attr = st.selectbox(
                    "Select Asset Class for Sensitivity Attribution",
                    options=relevant_asset_classes,
                    key='attr_asset_class_select'
                )
                if selected_asset_class_attr:
                    plot_sensitivity_attribution(raw_dvar_curr, current_day_date_map, selected_asset_class_attr)
                else:
                    st.info("Please select an asset class to view sensitivity attribution.")
            else:
                 st.info("No relevant asset classes (fx, rates, em macro) found in 'Current Day' DVaR data for sensitivity attribution.")
        else:
            st.info("Raw DVaR data is not available for sensitivity attribution.")


    with tab6: # Changed from tab7 to tab6
        st.header("‚öñÔ∏è SVaR vs. DVaR Comparison")
        st.markdown("Compare the Stressed VaR (SVaR) against the Diversified VaR (DVaR) to understand stress uplift. Comparisons are made for corresponding 'Current Day' or 'Previous Day' data.")
        
        comparison_type = st.radio(
            "Select Comparison Data Type",
            ('Current Day COB', 'Previous Day COB'),
            key='svar_dvar_compare_type'
        )

        dvar_compare_df = macro_dvar_curr if comparison_type == 'Current Day COB' else macro_dvar_prev
        svar_compare_df = macro_svar_curr if comparison_type == 'Current Day COB' else macro_svar_prev

        if not svar_compare_df.empty and not dvar_compare_df.empty:
            plot_svar_dvar_comparison(dvar_compare_df, svar_compare_df, f"Macro SVaR vs. Macro DVaR ({comparison_type})")
            
            # Calculate and display SVaR/DVaR ratio
            merged_var_data = pd.merge(dvar_compare_df[['Date', 'Macro_DVaR_Value', 'Pnl_Vector_Name']],
                                       svar_compare_df[['Date', 'Macro_SVaR_Value', 'Pnl_Vector_Name']],
                                       on=['Date', 'Pnl_Vector_Name'], how='inner')
            
            if not merged_var_data.empty:
                # Avoid division by zero and inf values
                merged_var_data['SVaR_DVaR_Ratio'] = merged_var_data.apply(
                    lambda row: row['Macro_SVaR_Value'] / row['Macro_DVaR_Value'] if row['Macro_DVaR_Value'] != 0 else np.nan,
                    axis=1
                )
                merged_var_data.replace([np.inf, -np.inf], np.nan, inplace=True)
                merged_var_data.dropna(subset=['SVaR_DVaR_Ratio'], inplace=True)

                if not merged_var_data.empty:
                    st.subheader(f"SVaR / DVaR Ratio Statistics ({comparison_type})")
                    st.dataframe(merged_var_data['SVaR_DVaR_Ratio'].describe())
                    st.info("A higher ratio indicates greater sensitivity to stress conditions.")
                else:
                    st.info(f"No valid SVaR/DVaR ratio data for {comparison_type} after cleaning.")
            else:
                st.info(f"No overlapping data for SVaR/DVaR ratio calculation for {comparison_type}.")

        else:
            st.info(f"No SVaR or DVaR data available for {comparison_type} comparison. "
                    "Ensure sheets are correctly named and contain data.")

    # Backtesting Tab (Placeholder - requires Actual PnL)
    # st.tabs(...) # If you want to add this, you'd add another tab here
    # with tabX: 
    #     st.header("üö¶ DVaR Backtesting (Requires Actual PnL)")
    #     st.warning("This section requires actual realized daily PnL data for backtesting.")
    #     st.write(
    #         """
    #         To perform DVaR backtesting, you need to provide a column in your data
    #         that represents the actual daily profit or loss for the same period.
    #         """
    #     )
    #     st.write("Please specify how to get actual PnL (e.g., specific column in `current_day_df`).")
    #     st.info("Once actual PnL is integrated, we can implement Kupiec's POF test and plot exceptions.")


else:
    st.info("Please upload your Excel file ('Tail_analysis_auto.xlsx') to start your DVaR analysis.")
