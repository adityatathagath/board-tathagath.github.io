import streamlit as st
import pandas as pd
import altair as alt
import io
import numpy as np # For statistical calculations like rolling std, correlations
from st_aggrid import AgGrid, GridOptionsBuilder, GridUpdateMode, DataReturnMode, JsCode

# --- Configuration (UPDATE THESE BASED ON YOUR DATA) ---
# IMPORTANT: Replace with your actual sheet names from the Excel workbook
CURRENT_DAY_SHEET_NAME = "DVaR_COB" # DVaR Current COB data
PREVIOUS_DAY_SHEET_NAME = "DVaR_Prev_COB" # DVaR Previous COB data
SVAR_COB_SHEET_NAME = "SVaR_COB" # SVaR Current COB data
SVAR_PREV_COB_SHEET_NAME = "SVaR_Prev_COB" # SVaR Previous COB data

# Node value for DVaR calculations for each asset class
# CONFIRM THESE VALUES!
FX_DVAR_NODE = 10
RATES_DVAR_NODE = 22194 # Corrected Node for Rates
EM_MACRO_DVAR_NODE = 137354 # Corrected Node for EM Macro

# PnL Vector range (assuming 260 days from 261 to 520)
PNL_VECTOR_START = 261
PNL_VECTOR_END = 520 # pnl_vector520 is inclusive, covers 260 days

# Define Barclays-specific color palette (example professional blue/grey tones)
# These colors are used for Altair charts.
BARCLAYS_COLOR_PALETTE = [
    '#0076B6',  # Primary Blue (Barclays blue)
    '#2188D7',  # Lighter Blue
    '#004B7F',  # Darker Blue
    '#6A6C6E',  # Medium Grey
    '#A0A3A6',  # Light Grey
    '#FF4B4B',  # Red for negative (used for change in tables, can be for trends too)
    '#28a745',  # Green for positive (used for change in tables, can be for trends too)
]

# --- Streamlit Application Setup ---
st.set_page_config(layout="wide", page_title="Market Risk DVaR Tail Analysis")

st.title("📊 Comprehensive Market Risk DVaR Tail Analysis")

st.write(
    """
    Explore trends, contributions, and top/bottom tails of your DVaR and SVaR data.
    Upload your Excel workbook using the sidebar on the left.
    """
)

# --- Sidebar for File Uploader and Debug Toggle ---
with st.sidebar:
    st.header("Upload Data & Debug Options")
    uploaded_file = st.file_uploader("Choose your Excel file (Tail_analysis_auto.xlsx)", type="xlsx")
    DEBUG_MODE = st.checkbox("Show Debug Info (for troubleshooting)", value=False)


# --- Helper Functions ---

@st.cache_data # Cache the data loading to avoid re-running on every interaction
def load_data(file_buffer, debug_mode):
    """
    Loads data from the specified sheets in the Excel workbook.
    Handles date extraction from the first row and sets proper headers.
    Ensures 'Node' column is numeric.
    Includes debug output.
    """
    data_frames = {}
    date_mappings = {}

    # List of all sheets to load
    sheets_to_load = [
        CURRENT_DAY_SHEET_NAME,
        PREVIOUS_DAY_SHEET_NAME,
        SVAR_COB_SHEET_NAME,
        SVAR_PREV_COB_SHEET_NAME
    ]

    for sheet_name in sheets_to_load:
        if debug_mode:
            st.sidebar.subheader(f"DEBUG: Loading Sheet: {sheet_name}")
        try:
            # IMPORTANT: Reset file_buffer position for each sheet read
            file_buffer.seek(0) 

            # Read the first two rows to get dates and column names
            raw_df_header_dates = pd.read_excel(file_buffer, sheet_name=sheet_name, header=None, nrows=2)
            
            # --- CRITICAL ERROR CHECK: Ensure enough rows for headers ---
            if len(raw_df_header_dates) < 2:
                raise ValueError(f"Sheet '{sheet_name}' does not have at least 2 header rows. "
                                 "Expected first row for dates, second for column names.")

            if debug_mode:
                st.sidebar.write(f"DEBUG: Raw first two rows of '{sheet_name}':")
                st.sidebar.dataframe(raw_df_header_dates)

            # The first row contains dates, the second row contains actual column names
            dates_row = raw_df_header_dates.iloc[0]
            column_names_row = raw_df_header_dates.iloc[1]

            if debug_mode:
                st.sidebar.write(f"DEBUG: Dates Row from '{sheet_name}':")
                st.sidebar.write(dates_row)
                st.sidebar.write(f"DEBUG: Column Names Row from '{sheet_name}':")
                st.sidebar.write(column_names_row)


            # Read the actual data, skipping the first two rows
            file_buffer.seek(0) # Reset buffer again before reading full data
            df = pd.read_excel(file_buffer, sheet_name=sheet_name, header=None, skiprows=2)
            df.columns = column_names_row # Assign the second row as column headers
            
            # Drop columns that are entirely NaN after header adjustments (e.g., empty columns in Excel)
            df = df.dropna(axis=1, how='all')

            if debug_mode:
                st.sidebar.write(f"DEBUG: DataFrame '{sheet_name}' after initial load and column assignment (head):")
                st.sidebar.dataframe(df.head())
                st.sidebar.write(f"DEBUG: DataFrame '{sheet_name}' columns:")
                st.sidebar.write(df.columns.tolist())


            # Ensure 'Node' column is numeric for consistent filtering
            if 'Node' in df.columns:
                df['Node'] = pd.to_numeric(df['Node'], errors='coerce')
                df['Node'] = df['Node'].astype('Int64') # Using nullable integer type
                if debug_mode:
                    st.sidebar.write(f"DEBUG: 'Node' column dtypes for '{sheet_name}': {df['Node'].dtype}")
            else:
                if debug_mode:
                    st.sidebar.warning(f"DEBUG: 'Node' column not found in sheet '{sheet_name}'. This might cause issues.")


            # Create mapping for pnl_vector columns to their dates
            pnl_date_map = {}
            for col_idx, col_name in enumerate(column_names_row):
                if pd.isna(col_name): # Skip NaN column names
                    continue
                # Handle both pnl_vectorXXX and pnl_vector[T-2] formats
                if str(col_name).startswith('pnl_vector') or ('[T-2]' in str(col_name) and 'pnl_vector' in str(col_name)): 
                    # Ensure col_idx is within bounds of dates_row
                    if col_idx < len(dates_row):
                        date_val = dates_row.iloc[col_idx]
                        # Convert date_val to datetime, handle potential Excel float dates
                        if isinstance(date_val, (int, float)):
                            try:
                                # Convert Excel float date to datetime
                                # Origin is 1899-12-30 for Excel dates (number of days since 1899-12-30)
                                pnl_date_map[str(col_name)] = pd.to_datetime(date_val, unit='D', origin='1899-12-30')
                            except:
                                pnl_date_map[str(col_name)] = pd.NaT # Not a Time, if conversion fails
                        else:
                            pnl_date_map[str(col_name)] = pd.to_datetime(date_val, errors='coerce') # Coerce errors to NaT
                    else:
                        pnl_date_map[str(col_name)] = pd.NaT # No date found for this column
            
            if debug_mode:
                st.sidebar.write(f"DEBUG: PnL Date Map for '{sheet_name}':")
                st.sidebar.write(pnl_date_map)

            data_frames[sheet_name] = df
            date_mappings[sheet_name] = pnl_date_map

        except Exception as e:
            st.sidebar.error(f"Error loading data from sheet '{sheet_name}': {e}. "
                             "Please ensure the sheet names are correct and the first two rows contain dates/headers as expected.")
            return None, None
    
    return data_frames, date_mappings


def calculate_var_tails(df, pnl_date_map, sheet_type="current", var_type_filter="DVaR", debug_mode=False):
    """
    Calculates VaR tails (DVaR or SVaR) for FX, Rates, EM Macro, and Macro.
    Adds a 'Sheet_Type' column for differentiation.
    Includes debug output.
    """
    if debug_mode:
        st.sidebar.subheader(f"DEBUG: Calculating VaR Tails for {sheet_type} ({var_type_filter})")
        st.sidebar.write("DEBUG: Input DataFrame Head:")
        st.sidebar.dataframe(df.head())
        st.sidebar.write("DEBUG: Input DataFrame Columns:")
        st.sidebar.write(df.columns.tolist())
        st.sidebar.write("DEBUG: Input PnL Date Map:")
        st.sidebar.write(pnl_date_map)

    # Define common identifying variables
    id_vars = ['Var Type', 'Node', 'Asset class', 'currency', 'sensitivity_type', 'load_code']
    
    # Identify all PnL vector columns that are actual columns in the dataframe
    pnl_vector_cols = [col for col in df.columns if str(col).startswith('pnl_vector') or ('[T-2]' in str(col) and 'pnl_vector' in str(col))]

    valid_pnl_cols = []
    for col in pnl_vector_cols:
        col_str = str(col)
        # DVaR Current COB (pnl_vector261 to pnl_vector520, no [T-2])
        if sheet_type == "current" and var_type_filter == "DVaR":
            if col_str.startswith('pnl_vector') and not '[T-2]' in col_str:
                try:
                    num_part = col_str.replace('pnl_vector', '')
                    if num_part.isdigit(): # Ensure it's purely a number
                        num = int(num_part)
                        if PNL_VECTOR_START <= num <= PNL_VECTOR_END:
                            valid_pnl_cols.append(col)
                except ValueError:
                    continue # Skip if not a valid number after stripping prefix
        # DVaR Previous COB (pnl_vectorXXX[T-2])
        elif sheet_type == "previous" and var_type_filter == "DVaR":
            if col_str.startswith('pnl_vector') and '[T-2]' in col_str:
                valid_pnl_cols.append(col) # All columns with [T-2] suffix
        # SVaR Current COB (pnl_vector1, pnl_vector2, etc., no [T-2])
        elif sheet_type == "current" and var_type_filter == "SVaR":
            if col_str.startswith('pnl_vector') and not '[T-2]' in col_str:
                valid_pnl_cols.append(col) # All pnl_vector columns (e.g., 1, 2, 3...)
        # SVaR Previous COB (pnl_vector1[T-2], etc.)
        elif sheet_type == "previous" and var_type_filter == "SVaR":
            if col_str.startswith('pnl_vector') and '[T-2]' in col_str:
                valid_pnl_cols.append(col) # All pnl_vector[T-2] columns
    
    if debug_mode:
        st.sidebar.write(f"DEBUG: Detected PnL Vector Columns: {pnl_vector_cols}")
        st.sidebar.write(f"DEBUG: Valid PnL Columns for current config ({sheet_type}, {var_type_filter}): {valid_pnl_cols}")


    # Ensure all id_vars are present in the dataframe columns before melting
    id_vars_present = [col for col in id_vars if col in df.columns]
    
    # Melt only if valid pnl columns exist
    if not valid_pnl_cols:
        st.warning(f"No valid PnL vector columns found for {sheet_type} ({var_type_filter}) data. Skipping VaR calculation.")
        # Return empty DataFrames instead of None for easier subsequent handling
        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()

    df_melted = df.melt(id_vars=id_vars_present,
                         value_vars=valid_pnl_cols,
                         var_name='Pnl_Vector_Name',
                         value_name='Value')
    
    # --- Add Pnl_Vector_Rank ---
    # Improved logic to handle [T-2] suffix for rank extraction
    def extract_pnl_rank(pnl_vector_name):
        # Remove '[T-2]' suffix if present, then extract digits
        name_without_suffix = pnl_vector_name.split('[T-2]')[0]
        numeric_part = ''.join(filter(str.isdigit, name_without_suffix))
        return int(numeric_part) if numeric_part else np.nan
    
    df_melted['Pnl_Vector_Rank'] = df_melted['Pnl_Vector_Name'].apply(extract_pnl_rank)
    df_melted['Pnl_Vector_Rank'] = df_melted['Pnl_Vector_Rank'].astype('Int64') # Nullable integer

    if debug_mode:
        st.sidebar.write("DEBUG: DataFrame after melting (head):")
        st.sidebar.dataframe(df_melted.head())
        st.sidebar.write("DEBUG: DataFrame after melting (shape):", df_melted.shape)
        st.sidebar.write("DEBUG: Pnl_Vector_Rank values (sample):", df_melted['Pnl_Vector_Rank'].dropna().unique())
    
    # Add a 'Date' column using the pnl_date_map
    df_melted['Date'] = df_melted['Pnl_Vector_Name'].map(pnl_date_map)
    df_melted['Date'] = pd.to_datetime(df_melted['Date'], errors='coerce') # Coerce errors for NaT dates
    
    # Filter out rows where Date is NaT (if date mapping failed for some columns)
    df_melted = df_melted.dropna(subset=['Date'])
    
    if debug_mode:
        st.sidebar.write("DEBUG: DataFrame after Date mapping and dropping NaT Dates (head):")
        st.sidebar.dataframe(df_melted.head())
        st.sidebar.write("DEBUG: DataFrame after Date mapping and dropping NaT Dates (shape):", df_melted.shape)


    # Filter for the specific Var Type (DVaR or SVaR) as the sheets are now distinct
    df_filtered_var_type = df_melted[df_melted['Var Type'] == var_type_filter].copy()
    
    if debug_mode:
        st.sidebar.write(f"DEBUG: DataFrame after filtering for 'Var Type' == '{var_type_filter}' (head):")
        st.sidebar.dataframe(df_filtered_var_type.head())
        st.sidebar.write(f"DEBUG: DataFrame after filtering for 'Var Type' == '{var_type_filter}' (shape):", df_filtered_var_type.shape)

    # --- Calculate VaR for each asset class ---
    # Corrected asset_classes list to match user's explicit casing
    asset_classes = ['FX', 'Rates', 'EM Macro']
    asset_var_dfs = {}
    
    node_mapping = {
        'FX': FX_DVAR_NODE, 
        'Rates': RATES_DVAR_NODE, 
        'EM Macro': EM_MACRO_DVAR_NODE
    }

    for ac in asset_classes:
        # Ensure comparison with nullable integer type for Node
        node_val_for_filter = node_mapping.get(ac, FX_DVAR_NODE) # Default to FX Node if not found
        filtered_df = df_filtered_var_type[
            (df_filtered_var_type['Asset class'] == ac) & # Use corrected casing for filtering
            (df_filtered_var_type['Node'] == node_val_for_filter)
        ]
        if debug_mode:
            st.sidebar.write(f"DEBUG: Filtered data for Asset Class '{ac}' and Node '{node_val_for_filter}' (head):")
            st.sidebar.dataframe(filtered_df.head())
            st.sidebar.write(f"DEBUG: Filtered data for Asset Class '{ac}' and Node '{node_val_for_filter}' (shape):", filtered_df.shape)

        if not filtered_df.empty:
            asset_var_dfs[ac] = filtered_df.groupby(['Date', 'Pnl_Vector_Name', 'Pnl_Vector_Rank'])['Value'].sum().reset_index(name=f'{ac.replace(" ", "_")}_{var_type_filter}_Value')
            asset_var_dfs[ac]['Sheet_Type'] = sheet_type # Add sheet type for merging
        else:
            # Ensure an empty DataFrame with expected columns if no data found
            asset_var_dfs[ac] = pd.DataFrame(columns=['Date', 'Pnl_Vector_Name', 'Pnl_Vector_Rank', f'{ac.replace(" ", "_")}_{var_type_filter}_Value', 'Sheet_Type'])
            
    # --- Combine all VaR tails for Macro VaR ---
    macro_var_df = pd.DataFrame(columns=['Date', 'Pnl_Vector_Name', 'Pnl_Vector_Rank', f'Macro_{var_type_filter}_Value', 'Sheet_Type']) # Initialize empty DF
    
    # Check if 'FX' data exists before proceeding, as it's the first in the sum
    if 'FX' in asset_var_dfs and not asset_var_dfs['FX'].empty:
        macro_var_df = asset_var_dfs['FX']
        for ac in ['Rates', 'EM Macro']: # Use corrected casing
            if ac in asset_var_dfs and not asset_var_dfs[ac].empty:
                # Merge with Pnl_Vector_Rank as a key
                macro_var_df = pd.merge(macro_var_df, asset_var_dfs[ac], on=['Date', 'Pnl_Vector_Name', 'Pnl_Vector_Rank', 'Sheet_Type'], how='outer')
        
        # Calculate Macro VaR
        # Ensure all columns for sum exist, fillna(0) for safety in case of outer merge NaNs
        cols_for_sum = [f'{ac.replace(" ", "_")}_{var_type_filter}_Value' for ac in asset_classes]
        # Add missing columns to macro_var_df before sum, filling with 0
        for col_sum in cols_for_sum:
            if col_sum not in macro_var_df.columns:
                macro_var_df[col_sum] = 0
        
        macro_var_df[f'Macro_{var_type_filter}_Value'] = macro_var_df[cols_for_sum].sum(axis=1)
        macro_var_df = macro_var_df.sort_values('Date').reset_index(drop=True)
        macro_var_df['Sheet_Type'] = sheet_type # Ensure this column is consistent
    else:
        if debug_mode:
            st.sidebar.write("DEBUG: Macro DVaR calculation skipped as FX data is empty.")

    if debug_mode:
        st.sidebar.write(f"DEBUG: Final Macro DVaR DF ({sheet_type}, {var_type_filter}) head and shape:")
        st.sidebar.dataframe(macro_var_df.head())
        st.sidebar.write(macro_var_df.shape)

    # Return individual asset class VaR DFs, macro VaR DF, and the raw filtered VaR DF for sensitivity analysis
    return asset_var_dfs.get('FX', pd.DataFrame()), \
           asset_var_dfs.get('Rates', pd.DataFrame()), \
           asset_var_dfs.get('EM Macro', pd.DataFrame()), \
           macro_var_df, \
           df_filtered_var_type # This is the raw data filtered by Var Type for sensitivity analysis

# --- Visualization Functions (No changes needed, they use generic column names for plots) ---

def plot_dvar_trends(df, title, y_column, legend_title="Type"):
    """Generates a line chart for DVaR trends."""
    if df.empty: return # Added guard
    chart = alt.Chart(df).mark_line(point=True).encode(
        x=alt.X('Date:T', title='Date', axis=alt.Axis(format='%d-%m-%Y')), # Date format change
        y=alt.Y(y_column, title='DVaR Value'),
        color=alt.Color('Sheet_Type:N', title=legend_title, scale=alt.Scale(range=BARCLAYS_COLOR_PALETTE)), # Apply custom palette
        tooltip=[alt.Tooltip('Date:T', format='%d-%m-%Y'), y_column, 'Sheet_Type', 'Pnl_Vector_Rank'] # Date format change, added rank
    ).properties(
        title=title
    ).interactive() # Enable zooming and panning
    st.altair_chart(chart, use_container_width=True)

def plot_dvar_volatility(df, title):
    """Generates a line chart for rolling DVaR volatility."""
    if df.empty or 'Rolling_Std_DVaR' not in df.columns: # Added guard
        st.warning("Rolling Standard Deviation column not found or DataFrame is empty for volatility plot.")
        return

    chart = alt.Chart(df).mark_line(point=True).encode(
        x=alt.X('Date:T', title='Date', axis=alt.Axis(format='%d-%m-%Y')), # Date format change
        y=alt.Y('Rolling_Std_DVaR', title='Rolling Std Dev of DVaR'),
        color=alt.Color('Sheet_Type:N', title='Data Type', scale=alt.Scale(range=BARCLAYS_COLOR_PALETTE)), # Apply custom palette
        tooltip=[alt.Tooltip('Date:T', format='%d-%m-%Y'), 'Rolling_Std_DVaR', 'Sheet_Type'] # Date format change
    ).properties(
        title=title
    ).interactive()
    st.altair_chart(chart, use_container_width=True)

def plot_contribution(df, title):
    """Generates a stacked area chart for asset class DVaR contributions."""
    if df.empty: 
        st.info(f"No data available for {title} chart.")
        return # Added guard
    df_long = df.melt(id_vars=['Date', 'Pnl_Vector_Name', 'Macro_DVaR_Value', 'Sheet_Type'],
                      value_vars=[col for col in df.columns if '_DVaR_Value' in col and col != 'Macro_DVaR_Value'],
                      var_name='Asset_Class',
                      value_name='Contribution_Value')
    
    # Ensure Macro_DVaR_Value is not zero to avoid division by zero
    # Handle cases where Total_DVaR is zero, set contribution to 0 if individual contribution is also 0.
    df_long['Contribution_Percentage'] = np.where(
        (df_long['Macro_DVaR_Value'] != 0),
        (df_long['Contribution_Value'] / df_long['Macro_DVaR_Value']) * 100,
        np.where(df_long['Contribution_Value'] == 0, 0, np.nan) # If Macro_DVaR_Value is 0, and Contribution_Value is also 0, then 0. Else NaN.
    )
    
    df_long = df_long.dropna(subset=['Contribution_Percentage'])

    if df_long.empty: # Added guard after dropna
        st.info(f"No valid contribution data after filtering for {title}.")
        return

    chart = alt.Chart(df_long).mark_area().encode(
        x=alt.X('Date:T', title='Date', axis=alt.Axis(format='%d-%m-%Y')), # Date format change
        y=alt.Y('Contribution_Percentage', title='Percentage Contribution (%)', stack='normalize'),
        color=alt.Color('Asset_Class:N', title='Asset Class', scale=alt.Scale(range=BARCLAYS_COLOR_PALETTE)), # Apply custom palette
        tooltip=[alt.Tooltip('Date:T', format='%d-%m-%Y'), 'Asset_Class:N', alt.Tooltip('Contribution_Percentage', format='.2f%')] # CORRECTED LINE
    ).properties(
        title=title
    ).interactive()
    st.altair_chart(chart, use_container_width=True)

def display_correlations(df):
    """Calculates and displays correlation matrix for DVaR asset classes."""
    if df.empty:
        st.info("No data available to calculate correlations.")
        return

    # Select only the DVaR value columns for current day data for correlation
    df_current_day = df[df['Sheet_Type'] == 'current']
    if df_current_day.empty:
        st.info("No 'Current Day' data available for correlation calculation.")
        return

    # Filter for columns that contain the DVaR values for correlation
    dvar_value_cols = [col for col in df_current_day.columns if '_DVaR_Value' in col and col != 'Macro_DVaR_Value']
    
    if not dvar_value_cols:
        st.info("No individual asset class DVaR values found for correlation calculation.")
        return

    correlation_df = df_current_day[dvar_value_cols].corr()
    st.subheader("DVaR Asset Class Correlations (Current Day)")
    st.dataframe(correlation_df.style.background_gradient(cmap='viridis', axis=None).format("{:.2f}"))
    st.markdown("Higher positive values indicate stronger positive correlation, negative values indicate inverse correlation.")


def plot_exceedances(df, title, threshold):
    """Plots DVaR values and highlights exceedances."""
    if threshold is None or df.empty:
        st.warning("Please set a threshold and ensure data is available for exceedance analysis.")
        return

    df_plot = df.copy() # Work on a copy to avoid SettingWithCopyWarning
    df_plot['Exceeds_Threshold'] = df_plot['Macro_DVaR_Value'] > threshold

    chart = alt.Chart(df_plot).mark_line().encode(
        x=alt.X('Date:T', title='Date', axis=alt.Axis(format='%d-%m-%Y')), # Date format change
        y=alt.Y('Macro_DVaR_Value', title='Macro DVaR Value'),
        tooltip=[alt.Tooltip('Date:T', format='%d-%m-%Y'), 'Macro_DVaR_Value'] # Date format change
    ).properties(
        title=title
    )

    exceedance_points = alt.Chart(df_plot[df_plot['Exceeds_Threshold']]).mark_point(color='red', size=100).encode(
        x=alt.X('Date:T'),
        y=alt.Y('Macro_DVaR_Value'),
        tooltip=[alt.Tooltip('Date:T', format='%d-%m-%Y'), 'Macro_DVaR_Value', {'title': 'Status', 'value': 'Exceeded Threshold'}] # CORRECTED LINE
    )

    threshold_line = alt.Chart(pd.DataFrame({'threshold': [threshold]})).mark_rule(color='red', strokeDash=[5,5]).encode(
        y='threshold'
    )

    st.altair_chart(chart + exceedance_points + threshold_line, use_container_width=True)
    st.info(f"Number of times Macro DVaR exceeded {threshold:.2f}: {df_plot['Exceeds_Threshold'].sum()}")


def plot_svar_dvar_comparison(dvar_df, svar_df, title):
    """Compares Macro DVaR and Macro SVaR."""
    if svar_df.empty or dvar_df.empty:
        st.info("SVaR or DVaR data not available for comparison.")
        return
    
    # Ensure we are comparing 'current' vs 'current' or 'previous' vs 'previous'
    # Merge DVaR and SVaR on Date and Pnl_Vector_Name
    comparison_df = pd.merge(dvar_df[['Date', 'Pnl_Vector_Name', 'Macro_DVaR_Value', 'Sheet_Type']],
                             svar_df[['Date', 'Pnl_Vector_Name', 'Macro_SVaR_Value', 'Sheet_Type']],
                             on=['Date', 'Pnl_Vector_Name', 'Sheet_Type'], how='inner')
    
    if comparison_df.empty:
        st.info("No common dates/pnl vectors found between DVaR and SVaR data for comparison. "
                "Ensure both sheets have corresponding entries and dates.")
        return

    # Melt for Altair
    df_melted = comparison_df.melt(id_vars=['Date', 'Pnl_Vector_Name', 'Sheet_Type'],
                                   value_vars=['Macro_DVaR_Value', 'Macro_SVaR_Value'],
                                   var_name='VaR_Type',
                                   value_name='Value')

    chart = alt.Chart(df_melted).mark_line(point=True).encode(
        x=alt.X('Date:T', title='Date', axis=alt.Axis(format='%d-%m-%Y')), # Date format change
        y=alt.Y('Value', title='VaR Value'),
        color=alt.Color('VaR_Type:N', title='VaR Type', scale=alt.Scale(range=BARCLAYS_COLOR_PALETTE)), # Apply custom palette
        strokeDash='VaR_Type:N', # Different line styles for clarity
        tooltip=[alt.Tooltip('Date:T', format='%d-%m-%Y'), 'VaR_Type', 'Value', 'Sheet_Type'] # Date format change
    ).properties(
        title=title
    ).interactive()
    st.altair_chart(chart, use_container_width=True)


def plot_sensitivity_attribution(df_raw_var_type, pnl_date_map, selected_asset_class):
    """
    Calculates and plots VaR contribution by sensitivity type for a selected asset class.
    df_raw_var_type is the full raw df for the specific VAR type (DVaR or SVaR).
    """
    if df_raw_var_type.empty:
        st.info("Raw VaR data not available for sensitivity attribution.")
        return

    # Dynamically get the node value for the selected asset class
    node_config_key = f"{selected_asset_class.replace(' ', '_').upper()}_DVAR_NODE"
    node_value_for_filter = globals().get(node_config_key)
    
    if node_value_for_filter is None:
        st.error(f"Configuration error: Node value for '{selected_asset_class}' (expected key: {node_config_key}) is not defined in the script's configuration section. Please check the `FX_DVAR_NODE`, `RATES_DVAR_NODE`, `EM_MACRO_DVAR_NODE` definitions.")
        return

    # Filter data for the selected asset class and node
    df_filtered = df_raw_var_type[
        (df_raw_var_type['Asset class'] == selected_asset_class) &
        (df_raw_var_type['Node'] == node_value_for_filter)
    ].copy()
    
    if df_filtered.empty:
        # Changed f-string to explicitly convert node_value_for_filter to string
        st.info(f"No DVaR data found for '{selected_asset_class}' with Node {str(node_value_for_filter)}. "
                "Please ensure this combination exists in your data and the node configuration is correct.")
        return

    # Group by Date, Pnl_Vector_Name, and Sensitivity Type to sum values
    sensitivity_contributions = df_filtered.groupby(['Date', 'Pnl_Vector_Name', 'sensitivity_type'])['Value'].sum().reset_index()
    
    # Calculate total DVaR for normalization
    total_dvar_by_date = sensitivity_contributions.groupby(['Date', 'Pnl_Vector_Name'])['Value'].sum().reset_index(name='Total_DVaR')
    
    # Merge total DVaR back
    sensitivity_contributions = pd.merge(sensitivity_contributions, total_dvar_by_date, on=['Date', 'Pnl_Vector_Name'], how='left')
    
    # Calculate percentage contribution
    # Handle cases where Total_DVaR is zero: set contribution to 0 if individual contribution is also 0, else NaN.
    sensitivity_contributions['Percentage_Contribution'] = np.where(
        (sensitivity_contributions['Total_DVaR'] != 0),
        (sensitivity_contributions['Value'] / sensitivity_contributions['Total_DVaR']) * 100,
        np.where(sensitivity_contributions['Value'] == 0, 0, np.nan)
    )
    
    sensitivity_contributions.replace([np.inf, -np.inf], np.nan, inplace=True)
    sensitivity_contributions.dropna(subset=['Percentage_Contribution'], inplace=True)

    if sensitivity_contributions.empty:
        st.info(f"No valid sensitivity contribution data after filtering for {selected_asset_class}.")
        return

    # Sort for consistent stacking
    sensitivity_contributions = sensitivity_contributions.sort_values(['Date', 'Pnl_Vector_Name', 'Percentage_Contribution'], ascending=[True, True, False])

    # Get top N sensitivities for display, group others
    top_n = st.slider(f"Show Top N Sensitivities for {selected_asset_class}", 5, 20, 10, key=f"top_n_sens_{selected_asset_class}")
    
    # Calculate average contribution of each sensitivity to identify top N
    # Use 'Value' for ranking as percentage can be misleading if total is very small
    avg_sens_contrib = sensitivity_contributions.groupby('sensitivity_type')['Value'].mean().nlargest(top_n).index
    
    sensitivity_contributions['Display_Sensitivity'] = sensitivity_contributions['sensitivity_type'].apply(
        lambda x: x if x in avg_sens_contrib else 'Other'
    )
    
    # Recalculate values and contributions for 'Other' category if it exists
    sensitivity_contributions_display = sensitivity_contributions.groupby(['Date', 'Pnl_Vector_Name', 'Display_Sensitivity'])['Value'].sum().reset_index()
    
    total_dvar_by_date_recalculated = sensitivity_contributions_display.groupby(['Date', 'Pnl_Vector_Name'])['Value'].sum().reset_index(name='Total_DVaR')
    sensitivity_contributions_display = pd.merge(sensitivity_contributions_display, total_dvar_by_date_recalculated, on=['Date', 'Pnl_Vector_Name'], how='left')
    
    sensitivity_contributions_display['Percentage_Contribution'] = (sensitivity_contributions_display['Value'] / sensitivity_contributions_display['Total_DVaR']) * 100
    
    sensitivity_contributions_display.replace([np.inf, -np.inf], np.nan, inplace=True)
    sensitivity_contributions_display.dropna(subset=['Percentage_Contribution'], inplace=True)

    if sensitivity_contributions_display.empty:
        st.info(f"No displayable sensitivity contribution data after grouping for {selected_asset_class}.")
        return

    # Ensure Date is datetime for Altair
    sensitivity_contributions_display['Date'] = pd.to_datetime(sensitivity_contributions_display['Date'])

    chart = alt.Chart(sensitivity_contributions_display).mark_area().encode(
        x=alt.X('Date:T', title='Date', axis=alt.Axis(format='%d-%m-%Y')), # Date format change
        y=alt.Y('Percentage_Contribution', title='Percentage Contribution (%)', stack='normalize'),
        color=alt.Color('Display_Sensitivity:N', title='Sensitivity Type', legend=alt.Legend(columns=2), scale=alt.Scale(range=BARCLAYS_COLOR_PALETTE)), # Apply custom palette
        order=alt.Order('Percentage_Contribution', sort='descending'),
        tooltip=[alt.Tooltip('Date:T', format='%d-%m-%Y'), 'Display_Sensitivity:N', alt.Tooltip('Percentage_Contribution', format='.2f%'), 'Value'] # Date format change
    ).properties(
        title=f"DVaR Contribution by Sensitivity Type for {selected_asset_class}"
    ).interactive()
    st.altair_chart(chart, use_container_width=True)

def display_top_bottom_tails_table(macro_dvar_curr, macro_dvar_prev, fx_dvar_curr, fx_dvar_prev, rates_dvar_curr, rates_dvar_prev, em_macro_dvar_curr, em_macro_dvar_prev, debug_mode):
    """
    Generates and displays the top 20 positive and negative Macro DVaR tails table using AgGrid.
    """
    st.header("🏆 Top/Bottom DVaR Tails Analysis")
    st.markdown("Identify the top 20 positive and negative Macro DVaR tails and their corresponding asset class contributions, showing change from previous COB.")

    # Common merge keys for all DVaR dataframes
    common_merge_keys = ['Date', 'Pnl_Vector_Name', 'Pnl_Vector_Rank']

    # --- Step 1: Identify Top/Bottom Tails from Current COB Macro DVaR ---
    if macro_dvar_curr.empty:
        st.info("No current Macro DVaR data to identify top/bottom tails.")
        return # Exit function if no data

    # Get top 20 positive and negative Macro DVaR values from current COB
    # Ensure to get copies to avoid SettingWithCopyWarning
    top_20_positive_curr = macro_dvar_curr.nlargest(min(20, len(macro_dvar_curr)), 'Macro_DVaR_Value', keep='all').copy()
    top_20_negative_curr = macro_dvar_curr.nsmallest(min(20, len(macro_dvar_curr)), 'Macro_DVaR_Value', keep='all').copy()

    # Combine these identified top/bottom tails into a single DataFrame for processing
    # This DataFrame will be the base for our final table, ensuring we only analyze these specific tails
    all_current_tails_base = pd.concat([top_20_positive_curr, top_20_negative_curr]).drop_duplicates(subset=common_merge_keys).reset_index(drop=True)
    
    # Rename Macro_DVaR_Value to Macro_DVaR_Value_Current immediately
    all_current_tails_base.rename(columns={'Macro_DVaR_Value': 'Macro_DVaR_Value_Current'}, inplace=True)

    if debug_mode:
        st.sidebar.write("DEBUG: All Current Tails (Top/Bottom Macro) for Lookup (head):")
        st.sidebar.dataframe(all_current_tails_base.head())
        st.sidebar.write("DEBUG: All Current Tails Columns:", all_current_tails_base.columns.tolist())


    # --- Step 2: Prepare lookup DataFrames for ALL previous COB asset class DVaR values ---
    # These will be used to left join against the identified current tails (all_current_tails_base)
    
    # List of (previous_df, previous_value_column_name, target_column_name_in_final_df)
    prev_lookup_configs = [
        (macro_dvar_prev, 'Macro_DVaR_Value', 'Macro_DVaR_Value_Previous'),
        (fx_dvar_prev, 'FX_DVaR_Value', 'FX_DVaR_Value_Previous'),
        (rates_dvar_prev, 'Rates_DVaR_Value', 'Rates_DVaR_Value_Previous'),
        (em_macro_dvar_prev, 'EM_Macro_DVaR_Value', 'EM_Macro_DVaR_Value_Previous')
    ]

    final_display_df = all_current_tails_base.copy() # Start with the identified current tails

    for prev_df, original_val_col, new_val_col in prev_lookup_configs:
        if not prev_df.empty and original_val_col in prev_df.columns:
            # Create a temporary DataFrame for merging, only with common keys and the relevant value column
            temp_prev_lookup_df = prev_df[common_merge_keys + [original_val_col]].copy()
            temp_prev_lookup_df.rename(columns={original_val_col: new_val_col}, inplace=True)
            
            # Left merge to pull in previous values. If no match, NaN will appear.
            final_display_df = pd.merge(final_display_df, temp_prev_lookup_df, on=common_merge_keys, how='left')
            
            if debug_mode:
                st.sidebar.write(f"DEBUG: Final Display DF after merging with {new_val_col} (head):")
                st.sidebar.dataframe(final_display_df.head())
        elif debug_mode:
            st.sidebar.write(f"DEBUG: Skipping previous lookup for {original_val_col} - DataFrame empty or column missing.")

    # Also, left merge current FX, Rates, EM_Macro values, as all_current_tails_base only has Macro_DVaR_Value_Current
    # It seems the initial top_20_positive_curr and negative_curr came directly from macro_dvar_curr.
    # We need to bring in current FX, Rates, EM_Macro values too.
    current_asset_lookup_configs = [
        (fx_dvar_curr, 'FX_DVaR_Value', 'FX_DVaR_Value_Current'),
        (rates_dvar_curr, 'Rates_DVaR_Value', 'Rates_DVaR_Value_Current'),
        (em_macro_dvar_curr, 'EM_Macro_DVaR_Value', 'EM_Macro_DVaR_Value_Current')
    ]
    for curr_df, original_val_col, new_val_col in current_asset_lookup_configs:
        if not curr_df.empty and original_val_col in curr_df.columns:
            temp_curr_lookup_df = curr_df[common_merge_keys + [original_val_col]].copy()
            temp_curr_lookup_df.rename(columns={original_val_col: new_val_col}, inplace=True)
            final_display_df = pd.merge(final_display_df, temp_curr_lookup_df, on=common_merge_keys, how='left')
        
        if debug_mode:
            st.sidebar.write(f"DEBUG: Final Display DF after merging with {new_val_col} (head):")
            st.sidebar.dataframe(final_display_df.head())

    if debug_mode:
        st.sidebar.write("DEBUG: Final Display DF after all asset-specific merges (before fillna and change calculation):")
        st.sidebar.dataframe(final_display_df.head())
        st.sidebar.write("DEBUG: Final Display DF columns after all asset-specific merges:", final_display_df.columns.tolist())


    # --- Step 3: Fill NaNs and Calculate Changes ---
    # Fill NaN values with 0 for all DVaR value columns (both Current and Previous)
    # This handles cases where data might be missing for a specific date/vector in one COB
    value_cols_to_fill = [col for col in final_display_df.columns if '_DVaR_Value_Current' in col or '_DVaR_Value_Previous' in col]
    final_display_df[value_cols_to_fill] = final_display_df[value_cols_to_fill].fillna(0)

    # Calculate Change columns
    asset_prefixes = ['Macro', 'FX', 'Rates', 'EM_Macro']
    for prefix in asset_prefixes:
        current_col_name = f'{prefix}_DVaR_Value_Current'
        previous_col_name = f'{prefix}_DVaR_Value_Previous'
        change_col_name = f'{prefix}_DVaR_Change'
        
        # Ensure the columns exist before calculating change; they should after fillna(0)
        if current_col_name in final_display_df.columns and previous_col_name in final_display_df.columns:
            final_display_df[change_col_name] = final_display_df[current_col_name] - final_display_df[previous_col_name]
        else:
            final_display_df[change_col_name] = 0 # Default change to 0 if columns missing


    # Sort final DataFrame for consistent display
    final_display_df = final_display_df.sort_values(by=['Date', 'Pnl_Vector_Rank']).reset_index(drop=True)

    if debug_mode:
        st.sidebar.write("DEBUG: Final Display DF ready for AgGrid (head):")
        st.sidebar.dataframe(final_display_df.head())
        st.sidebar.write("DEBUG: Final Display DF columns (final set):", final_display_df.columns.tolist())

    # --- Step 4: Prepare data for AgGrid display ---
    # Re-filter into top_20_positive and top_20_negative from the fully prepared final_display_df
    # This ensures consistency: the positive/negative selection is based on Macro_DVaR_Value_Current
    # from the final, fully merged and calculated DataFrame.
    top_20_positive_aggrid = final_display_df.nlargest(min(20, len(final_display_df)), 'Macro_DVaR_Value_Current', keep='all')
    top_20_negative_aggrid = final_display_df.nsmallest(min(20, len(final_display_df)), 'Macro_DVaR_Value_Current', keep='all')


    # Define AgGrid column definitions
    columnDefs = [
        {"field": "Date", "headerName": "Date", "type": ["dateColumnFilter", "customDateTimeFormat"], "custom_format_string": 'dd-MM-yyyy'},
        {"field": "Pnl_Vector_Name", "headerName": "P&L Vector"},
    ]

    # JavaScript code for cell styling (red for negative, green for positive)
    change_cell_style_jscode = JsCode(f"""
    function(params) {{
        if (params.value < 0) {{
            return {{backgroundColor: '{BARCLAYS_COLOR_PALETTE[6]}', color: 'black'}}; // Red background, black text
        }} else if (params.value > 0) {{
            return {{backgroundColor: '{BARCLAYS_COLOR_PALETTE[7]}', color: 'black'}}; // Green background, black text
        }}
        return null;
    }}
    """)
    
    # Format number with thousands separator and 2 decimal places
    # AgGrid also has a built-in cellRenderer for numbers, but custom JScode is flexible.
    number_formatter_jscode = JsCode("function(params) { return params.value.toLocaleString(undefined, {minimumFractionDigits: 2, maximumFractionDigits: 2}); }")

    asset_prefixes = ['Macro', 'FX', 'Rates', 'EM_Macro']
    for prefix in asset_prefixes:
        columnDefs.append({"field": f"{prefix}_DVaR_Value_Current", "headerName": f"{prefix} Current", "type": ["numericColumn", "numberColumnFilter"], "valueFormatter": number_formatter_jscode})
        columnDefs.append({"field": f"{prefix}_DVaR_Value_Previous", "headerName": f"{prefix} Previous", "type": ["numericColumn", "numberColumnFilter"], "valueFormatter": number_formatter_jscode})
        columnDefs.append({"field": f"{prefix}_DVaR_Change", "headerName": f"{prefix} Change", "type": ["numericColumn", "numberColumnFilter"], "cellStyle": change_cell_style_jscode, "valueFormatter": number_formatter_jscode})

    # Build GridOptions
    gb = GridOptionsBuilder.from_dataframe(pd.DataFrame(columns=[col['field'] for col in columnDefs])) 
    gb.configure_columns(columnDefs)
    gb.configure_grid_options(domLayout='autoHeight', suppressColumnVirtualization=True) # suppressColumnVirtualization can help with styling issues on large tables
    gridOptions = gb.build()


    st.subheader("Top 20 Positive Macro DVaR Tails (Current COB)")
    if not top_20_positive_aggrid.empty:
        top_20_positive_aggrid_display = top_20_positive_aggrid.copy()
        # Convert Date column to string for AgGrid (YYYY-MM-DD for custom_format_string to work reliably)
        top_20_positive_aggrid_display['Date'] = top_20_positive_aggrid_display['Date'].dt.strftime('%Y-%m-%d') 
        AgGrid(top_20_positive_aggrid_display, gridOptions=gridOptions, 
               data_return_mode='AS_INPUT', update_mode='MODEL_CHANGED', 
               fit_columns_on_grid_load=True, allow_unsafe_jscode=True, 
               enable_enterprise_modules=True, height=350, width='100%', reload_data=True, key='ag_pos_tails')
    else:
        st.info("No positive Macro DVaR tails found.")

    st.subheader("Top 20 Negative Macro DVaR Tails (Current COB)")
    if not top_20_negative_aggrid.empty:
        top_20_negative_aggrid_display = top_20_negative_aggrid.copy()
        # Convert Date column to string for AgGrid (YYYY-MM-DD for custom_format_string to work reliably)
        top_20_negative_aggrid_display['Date'] = top_20_negative_aggrid_display['Date'].dt.strftime('%Y-%m-%d') 
        AgGrid(top_20_negative_aggrid_display, gridOptions=gridOptions, 
               data_return_mode='AS_INPUT', update_mode='MODEL_CHANGED', 
               fit_columns_on_grid_load=True, allow_unsafe_jscode=True, 
               enable_enterprise_modules=True, height=350, width='100%', reload_data=True, key='ag_neg_tails')
    else:
        st.info("No negative Macro DVaR tails found.")


# --- Main Application Logic ---
# The uploaded_file and DEBUG_MODE are now retrieved from the sidebar
if uploaded_file is not None:
    data_sheets, date_mappings = load_data(uploaded_file, DEBUG_MODE) # Pass DEBUG_MODE

    if data_sheets is None or date_mappings is None:
        st.stop() # Stop if loading failed

    # Retrieve all four dataframes and their date mappings
    current_day_df = data_sheets.get(CURRENT_DAY_SHEET_NAME)
    previous_day_df = data_sheets.get(PREVIOUS_DAY_SHEET_NAME)
    svar_cob_df = data_sheets.get(SVAR_COB_SHEET_NAME)
    svar_prev_cob_df = data_sheets.get(SVAR_PREV_COB_SHEET_NAME)

    current_day_date_map = date_mappings.get(CURRENT_DAY_SHEET_NAME)
    previous_day_date_map = date_mappings.get(PREVIOUS_DAY_SHEET_NAME)
    svar_cob_date_map = date_mappings.get(SVAR_COB_SHEET_NAME)
    svar_prev_cob_date_map = date_mappings.get(SVAR_PREV_COB_SHEET_NAME)

    # Basic checks for loaded dataframes
    if not all([current_day_df is not None, previous_day_df is not None, 
                svar_cob_df is not None, svar_prev_cob_df is not None]):
        st.error("One or more required sheets could not be loaded. Please check sheet names and file content.")
        st.stop()
    if not all([current_day_date_map is not None, previous_day_date_map is not None,
                svar_cob_date_map is not None, svar_prev_cob_date_map is not None]):
        st.error("Date mappings could not be extracted for one or more sheets. Check the first row of your Excel sheets.")
        st.stop()

    st.success("Excel data loaded and dates extracted successfully!")

    with st.spinner("Calculating VaR tails... This may take a moment."):
        # Calculate DVaR for Current COB
        fx_dvar_curr, rates_dvar_curr, em_macro_dvar_curr, macro_dvar_curr, raw_dvar_curr = \
            calculate_var_tails(current_day_df, current_day_date_map, "current", "DVaR", DEBUG_MODE) # Pass DEBUG_MODE
        
        # Calculate DVaR for Previous COB
        fx_dvar_prev, rates_dvar_prev, em_macro_dvar_prev, macro_dvar_prev, raw_dvar_prev = \
            calculate_var_tails(previous_day_df, previous_day_date_map, "previous", "DVaR", DEBUG_MODE) # Pass DEBUG_MODE
        
        # Calculate SVaR for Current COB
        fx_svar_curr, rates_svar_curr, em_macro_svar_curr, macro_svar_curr, raw_svar_curr = \
            calculate_var_tails(svar_cob_df, svar_cob_date_map, "current", "SVaR", DEBUG_MODE) # Pass DEBUG_MODE

        # Calculate SVaR for Previous COB
        fx_svar_prev, rates_svar_prev, em_macro_svar_prev, macro_svar_prev, raw_svar_prev = \
            calculate_var_tails(svar_prev_cob_df, svar_prev_cob_date_map, "previous", "SVaR", DEBUG_MODE) # Pass DEBUG_MODE

    # Check if *any* macro_dvar_curr data exists to proceed with display
    if not macro_dvar_curr.empty: 
        st.success("DVaR and SVaR calculations complete for all sheets!")
        
        # --- Display Lowest DVaR and SVaR in Cards ---
        st.markdown("---")
        st.header("📉 Key Risk Metrics")
        
        col_dvar, col_svar = st.columns(2)

        # Lowest DVaR
        # Find the absolute minimum value for lowest tail, which could be a large negative number
        lowest_dvar_row = macro_dvar_curr.nsmallest(1, 'Macro_DVaR_Value').iloc[0] if not macro_dvar_curr.empty else None
        if lowest_dvar_row is not None:
            with col_dvar:
                st.metric(
                    label="Lowest Macro DVaR (Current COB)", 
                    value=f"{lowest_dvar_row['Macro_DVaR_Value']:,.2f}",
                    help=f"Date: {lowest_dvar_row['Date'].strftime('%d-%m-%Y')}, P&L Vector: {lowest_dvar_row['Pnl_Vector_Name']}"
                )
        else:
            with col_dvar:
                st.info("No DVaR data to display lowest metric.")
        
        # Lowest SVaR
        # Find the absolute minimum value for lowest tail, which could be a large negative number
        lowest_svar_row = macro_svar_curr.nsmallest(1, 'Macro_SVaR_Value').iloc[0] if not macro_svar_curr.empty else None
        if lowest_svar_row is not None:
            with col_svar:
                st.metric(
                    label="Lowest Macro SVaR (Current COB)", 
                    value=f"{lowest_svar_row['Macro_SVaR_Value']:,.2f}",
                    help=f"Date: {lowest_svar_row['Date'].strftime('%d-%m-%Y')}, P&L Vector: {lowest_svar_row['Pnl_Vector_Name']}"
                )
        else:
            with col_svar:
                st.info("No SVaR data to display lowest metric.")

        st.markdown("---") # Separator

        # Display Top/Bottom Tails table
        display_top_bottom_tails_table(macro_dvar_curr, macro_dvar_prev, fx_dvar_curr, fx_dvar_prev, rates_dvar_curr, rates_dvar_prev, em_macro_dvar_curr, em_macro_dvar_prev, DEBUG_MODE)
    else:
        st.warning("No DVaR data could be calculated. Please check your Excel file's format and content and enable debug mode for more details.")
        st.stop() # Stop execution if no DVaR data to display


    # --- Analysis Tabs ---
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "DVaR Trends", "Volatility", "Contribution", "Correlations",
        "Sensitivity Attribution", "SVaR Comparison"
    ])

    with tab1:
        st.header("📈 DVaR Time Series Trends")
        st.markdown("Visualize the evolution of Macro DVaR and individual Asset Class DVaRs over time.")

        # Concatenate DVaR data for "Current vs. Previous Day" trend plots
        all_macro_dvar = pd.concat([macro_dvar_curr, macro_dvar_prev], ignore_index=True) if not macro_dvar_curr.empty and not macro_dvar_prev.empty else (macro_dvar_curr if not macro_dvar_curr.empty else macro_dvar_prev)
        all_fx_dvar = pd.concat([fx_dvar_curr, fx_dvar_prev], ignore_index=True) if not fx_dvar_curr.empty and not fx_dvar_prev.empty else (fx_dvar_curr if not fx_dvar_curr.empty else fx_dvar_prev)
        all_rates_dvar = pd.concat([rates_dvar_curr, rates_dvar_prev], ignore_index=True) if not rates_dvar_curr.empty and not rates_dvar_prev.empty else (rates_dvar_curr if not rates_dvar_curr.empty else rates_dvar_prev)
        all_em_macro_dvar = pd.concat([em_macro_dvar_curr, em_macro_dvar_prev], ignore_index=True) if not em_macro_dvar_curr.empty and not em_macro_dvar_prev.empty else (em_macro_dvar_curr if not em_macro_dvar_curr.empty else em_macro_dvar_prev)


        if not all_macro_dvar.empty:
            plot_dvar_trends(all_macro_dvar, "Macro DVaR Trend (Current vs. Previous Day)", 'Macro_DVaR_Value')
            
            st.subheader("Individual Asset Class DVaR Trends")
            col1, col2, col3 = st.columns(3)
            with col1:
                if not all_fx_dvar.empty:
                    plot_dvar_trends(all_fx_dvar, "FX DVaR Trend", 'FX_DVaR_Value')
            with col2:
                if not all_rates_dvar.empty:
                    plot_dvar_trends(all_rates_dvar, "Rates DVaR Trend", 'Rates_DVaR_Value')
            with col3:
                if not all_em_macro_dvar.empty:
                    plot_dvar_trends(all_em_macro_dvar, "EM Macro DVaR Trend", 'EM_Macro_DVaR_Value')
        else:
            st.info("No DVaR data to display trends.")

    with tab2:
        st.header("📉 DVaR Volatility Analysis")
        st.markdown("Understand the stability of your DVaR over time by examining rolling standard deviation.")

        window_size = st.slider("Select Rolling Window Size (days)", 5, 60, 20, key='dvar_vol_window')
        
        if not all_macro_dvar.empty:
            # Calculate rolling std for Macro DVaR
            all_macro_dvar['Rolling_Std_DVaR'] = all_macro_dvar.groupby('Sheet_Type')['Macro_DVaR_Value'].transform(lambda x: x.rolling(window=window_size, min_periods=1).std())
            plot_dvar_volatility(all_macro_dvar, f"Macro DVaR Rolling Volatility ({window_size}-day window)")

            st.subheader("Individual Asset Class DVaR Volatility")
            col1, col2, col3 = st.columns(3)
            with col1:
                if not all_fx_dvar.empty:
                    all_fx_dvar['Rolling_Std_DVaR'] = all_fx_dvar.groupby('Sheet_Type')['FX_DVaR_Value'].transform(lambda x: x.rolling(window=window_size, min_periods=1).std())
                    plot_dvar_volatility(all_fx_dvar, f"FX DVaR Rolling Volatility ({window_size}-day window)")
            with col2:
                if not all_rates_dvar.empty:
                    all_rates_dvar['Rolling_Std_DVaR'] = all_rates_dvar.groupby('Sheet_Type')['Rates_DVaR_Value'].transform(lambda x: x.rolling(window=window_size, min_periods=1).std())
                    plot_dvar_volatility(all_rates_dvar, f"Rates DVaR Rolling Volatility ({window_size}-day window)")
            with col3:
                if not all_em_macro_dvar.empty:
                    all_em_macro_dvar['Rolling_Std_DVaR'] = all_em_macro_dvar.groupby('Sheet_Type')['EM_Macro_DVaR_Value'].transform(lambda x: x.rolling(window=window_size, min_periods=1).std())
                    plot_dvar_volatility(all_em_macro_dvar, f"EM Macro DVaR Rolling Volatility ({window_size}-day window)")
        else:
            st.info("No DVaR data to analyze volatility.")

    with tab3:
        st.header("🏛️ Asset Class Contribution to Macro DVaR")
        st.markdown("See how each asset class contributes to the overall Macro DVaR over time. Only for 'Current Day' DVaR data.")
        if not macro_dvar_curr.empty:
            plot_contribution(macro_dvar_curr, "Asset Class Contribution to Macro DVaR (Current Day)")
        else:
            st.info("No 'Current Day' Macro DVaR data to show contribution.")

    with tab4:
        st.header("🤝 Asset Class DVaR Correlations")
        st.markdown("Examine the correlation between DVaR values of different asset classes. Only for 'Current Day' DVaR data.")
        if not macro_dvar_curr.empty:
            display_correlations(macro_dvar_curr)
        else:
            st.info("No 'Current Day' DVaR data to calculate correlations.")


    with tab5: # Changed from tab6 to tab5
        st.header("🔬 DVaR Sensitivity Attribution")
        st.markdown("Break down DVaR by underlying sensitivity types within each asset class. Only for 'Current Day' DVaR data.")
        
        if raw_dvar_curr is not None and not raw_dvar_curr.empty:
            asset_class_options = raw_dvar_curr['Asset class'].unique().tolist()
            # Filter asset classes to ensure only 'FX', 'Rates', 'EM Macro' are options (corrected casing)
            relevant_asset_classes = [ac for ac in asset_class_options if ac in ['FX', 'Rates', 'EM Macro']]
            
            if relevant_asset_classes:
                selected_asset_class_attr = st.selectbox(
                    "Select Asset Class for Sensitivity Attribution",
                    options=relevant_asset_classes,
                    key='attr_asset_class_select'
                )
                if selected_asset_class_attr:
                    plot_sensitivity_attribution(raw_dvar_curr, current_day_date_map, selected_asset_class_attr)
                else:
                    st.info("Please select an asset class to view sensitivity attribution.")
            else:
                 st.info("No relevant asset classes (FX, Rates, EM Macro) found in 'Current Day' DVaR data for sensitivity attribution.")
        else:
            st.info("Raw DVaR data is not available for sensitivity attribution.")


    with tab6: # Changed from tab7 to tab6
        st.header("⚖️ SVaR vs. DVaR Comparison")
        st.markdown("Compare the Stressed VaR (SVaR) against the Diversified VaR (DVaR) to understand stress uplift. Comparisons are made for corresponding 'Current Day' or 'Previous Day' data.")
        
        comparison_type = st.radio(
            "Select Comparison Data Type",
            ('Current Day COB', 'Previous Day COB'),
            key='svar_dvar_compare_type'
        )

        dvar_compare_df = macro_dvar_curr if comparison_type == 'Current Day COB' else macro_dvar_prev
        svar_compare_df = macro_svar_curr if comparison_type == 'Current Day COB' else macro_svar_prev

        if not svar_compare_df.empty and not dvar_compare_df.empty:
            plot_svar_dvar_comparison(dvar_compare_df, svar_compare_df, f"Macro SVaR vs. Macro DVaR ({comparison_type})")
            
            # Calculate and display SVaR/DVaR ratio
            merged_var_data = pd.merge(dvar_compare_df[['Date', 'Macro_DVaR_Value', 'Pnl_Vector_Name']],
                                       svar_compare_df[['Date', 'Macro_SVaR_Value', 'Pnl_Vector_Name']],
                                       on=['Date', 'Pnl_Vector_Name'], how='inner')
            
            if not merged_var_data.empty:
                # Avoid division by zero and inf values
                merged_var_data['SVaR_DVaR_Ratio'] = merged_var_data.apply(
                    lambda row: row['Macro_SVaR_Value'] / row['Macro_DVaR_Value'] if row['Macro_DVaR_Value'] != 0 else np.nan,
                    axis=1
                )
                merged_var_data.replace([np.inf, -np.inf], np.nan, inplace=True)
                merged_var_data.dropna(subset=['SVaR_DVaR_Ratio'], inplace=True)

                if not merged_var_data.empty:
                    st.subheader(f"SVaR / DVaR Ratio Statistics ({comparison_type})")
                    st.dataframe(merged_var_data['SVaR_DVaR_Ratio'].describe())
                    st.info("A higher ratio indicates greater sensitivity to stress conditions.")
                else:
                    st.info(f"No valid SVaR/DVaR ratio data for {comparison_type} after cleaning.")
            else:
                st.info(f"No overlapping data for SVaR/DVaR ratio calculation for {comparison_type}.")

        else:
            st.info(f"No SVaR or DVaR data available for {comparison_type} comparison. "
                    "Ensure sheets are correctly named and contain data.")

    # Backtesting Tab (Placeholder - requires Actual PnL)
    # st.tabs(...) # If you want to add this, you'd add another tab here
    # with tabX: 
    #     st.header("🚦 DVaR Backtesting (Requires Actual PnL)")
    #     st.warning("This section requires actual realized daily PnL data for backtesting.")
    #     st.write(
    #         """
    #         To perform DVaR backtesting, you need to provide a column in your data
    #         that represents the actual daily profit or loss for the same period.
    #         """
    #     )
    #     st.write("Please specify how to get actual PnL (e.g., specific column in `current_day_df`).")
    #     st.info("Once actual PnL is integrated, we can implement Kupiec's POF test and plot exceptions.")


else:
    # Initial message when no file is uploaded yet
    st.info("Please upload your Excel file ('Tail_analysis_auto.xlsx') using the sidebar to start your DVaR analysis.")

