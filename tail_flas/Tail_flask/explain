#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EM Macro / FX / Rates — VaR Explainer (Hierarchy-aware)
======================================================

Purpose
-------
Given your drilldown export with hierarchical columns (e.g., `pl3..pl7`), this script
builds an executive-quality **VaR Explain** for a specific desk subtree — e.g.,
**EM Macro (node id 1373254 at pl3)** — including:

1) Executive Snapshot (portfolio VaR95/99, Δ vs T-2, ΔCoVar, Utilization)
2) Top Drivers at the *immediate child* level (e.g., Americas/APAC/EMEA under EM Macro)
3) Deep Drivers (leaf nodes) by |ΔVaR| (95 and 99) with directions
4) Regional roll-up (heuristics or optional mapping CSV)
5) Full branch dump for QA

Why hierarchy-aware?
--------------------
Parent rows (e.g., "1373254, EM Macro") have **non‑additive** VaR. We therefore:
- Use the **parent row itself** for the headline totals.
- Use *children & descendants* only to list **drivers** (not to sum to the parent).

Usage
-----
```bash
pip install pandas xlsxwriter

python var_explain_desk.py \
  --file "export.xlsx" \
  --sheet "Sheet1" \
  --node_id 1373254 \
  --node_level 3 \
  --out "VaR_Explain_EM_Macro.xlsx" \
  --topn 15 \
  --map "desk_to_region.csv"   # optional (Desk,Region)
```

Notes
-----
- `node_level` is the hierarchy level number that contains the node id (EM Macro = pl3).
- The script auto-detects hierarchy columns (`pl1..pl7`, `Drilldowns`, `Level1..N`).
- Metric column names are fuzzy-matched: `var95`, `var95 [T-2]`, `deltaCoVar95`, `var99`,
  `var99 [T-2]`, `deltaCoVar99`, and `LimitAmount`.
- Output sheets are fully formatted (integers with thousands separators, frozen panes, autofilters).
"""
import argparse
import re
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

# ------------------------------ Config ---------------------------------
SHEETS = {
    "summary": "Executive Summary",
    "drivers_child": "Top Drivers — Child Lv",
    "drivers_leaf95": "Leaf Drivers 95",
    "drivers_leaf99": "Leaf Drivers 99",
    "regions": "Regional Roll-up",
    "branch": "Branch (QA)",
}

REGION_RULES = [
    ("Americas", [r"\bamericas?\b", r"\bUS\b", r"\bU\.?S\.?\b", r"\bNY\b", r"\bmexic", r"\bLATAM\b", r"\bbr[az]il"]),
    ("APAC",     [r"\bAPAC\b", r"\bASIA\b", r"\bindia\b", r"\bsingapore\b", r"\bhong\s*kong\b", r"\bshanghai\b", r"\bPRC\b"]),
    ("EMEA",     [r"\bEMEA\b", r"\blondon\b", r"\bpoland\b", r"\bhuf\b", r"\bruk?b\b", r"\bce3\b", r"\bisrael\b", r"\brub\b", r"\bhungar"]),
]

COL_PATTERNS = {
    "limit":        [r"^limit.*amount$", r"^limit$", r"^limitamount$"],
    "var95":        [r"^mgt.*total.*var.*95$", r"^mgt.*var.*95$", r"^var\s*95$", r"^var95$"],
    "var95_t2":     [r"^.*var.*95.*t-?2.*$", r"^var95\[t-2\]$"],
    "deltacovar95": [r"^delta.*co.*var.*95$", r"^delta.*95$", r"^deltacovar95$"],
    "var99":        [r"^mgt.*total.*var.*99$", r"^mgt.*var.*99$", r"^var\s*99$", r"^var99$"],
    "var99_t2":     [r"^.*var.*99.*t-?2.*$", r"^var99\[t-2\]$"],
    "deltacovar99": [r"^delta.*co.*var.*99$", r"^delta.*99$", r"^deltacovar99$"],
}

# ------------------------------ Helpers --------------------------------
def nrm(s: str) -> str:
    return str(s).strip()

def pick_first_matching_column(columns: List[str], patterns: List[str]) -> Optional[str]:
    low = [c.lower() for c in columns]
    for pat in patterns:
        rx = re.compile(pat, re.I)
        for i, lc in enumerate(low):
            if rx.search(lc):
                return columns[i]
    return None

def detect_hierarchy_columns(df: pd.DataFrame) -> List[str]:
    tagged: List[Tuple[int, str]] = []
    for c in df.columns:
        cl = str(c).lower().strip()
        if cl.startswith("drilldown"):
            tagged.append((0, c))
            continue
        m = re.match(r"^(pl|p|level|lvl)\s*([0-9]+)$", cl) or re.match(r"^(pl|p|level|lvl)([0-9]+)$", cl)
        if m:
            tagged.append((int(m.group(2)), c))
    if not tagged:
        obj_cols = [c for c in df.columns if df[c].dtype == "object"]
        return obj_cols[:1] if obj_cols else [df.columns[0]]
    tagged.sort(key=lambda x: x[0])
    return [c for _, c in tagged]

def split_id_name(val: str) -> Tuple[Optional[str], Optional[str]]:
    if val is None:
        return None, None
    s = str(val).strip()
    if not s or s.lower() in ("nan", "none", "n/a", "na"):
        return None, None
    # Expect "12345, Desk Name" but be tolerant
    if "," in s:
        a, b = s.split(",", 1)
        a = a.strip()
        b = b.strip()
        return (a if a.isdigit() else None), b
    # If no comma, try extracting leading digits
    m = re.match(r"^(\d+)\s*(.*)$", s)
    if m:
        node = m.group(1)
        name = m.group(2).strip(" ,-") or None
        return node, name
    return None, s

def coerce_num(sr: pd.Series) -> pd.Series:
    return pd.to_numeric(sr, errors="coerce").fillna(0.0)

def infer_region(name: str) -> str:
    s = name.lower() if isinstance(name, str) else ""
    for region, pats in REGION_RULES:
        for pat in pats:
            if re.search(pat, s, re.I):
                return region
    return "Unmapped"

def find_metric_columns(df: pd.DataFrame) -> Dict[str, Optional[str]]:
    cols = [nrm(c) for c in df.columns]
    out: Dict[str, Optional[str]] = {}
    for k, pats in COL_PATTERNS.items():
        out[k] = pick_first_matching_column(cols, pats)
    return out

# ------------------------------ Core -----------------------------------

def build_branch_frames(df: pd.DataFrame, node_id: str, node_level: int,
                        region_map: Optional[pd.DataFrame] = None,
                        topn: int = 12) -> Dict[str, pd.DataFrame]:
    df = df.copy()
    df.columns = [nrm(c) for c in df.columns]

    # 1) Identify hierarchy columns and pull out id/name per level
    hier_cols = detect_hierarchy_columns(df)
    level_cols = {i+1: c for i, c in enumerate(hier_cols)}  # 1-based level

    for lvl, col in level_cols.items():
        ids, names = zip(*(split_id_name(v) for v in df[col]))
        df[f"L{lvl}_id"] = list(ids)
        df[f"L{lvl}_name"] = list(names)

    # 2) Metrics detection & coercion
    m = find_metric_columns(df)
    for key in ["var95","var95_t2","deltacovar95","var99","var99_t2","deltacovar99"]:
        df[key.upper()] = coerce_num(df[m[key]]) if m.get(key) else 0.0
    df["LIMIT"] = coerce_num(df[m["limit"]]) if m.get("limit") else 0.0

    df["dVaR95"] = df["VAR95"] - df["VAR95_T2"]
    df["dVaR99"] = df["VAR99"] - df["VAR99_T2"]
    df["Util95_%"] = np.where(df["LIMIT"]!=0, 100*df["VAR95"]/df["LIMIT"], np.nan)
    df["Util99_%"] = np.where(df["LIMIT"]!=0, 100*df["VAR99"]/df["LIMIT"], np.nan)

    # 3) Filter to the branch (keep parent row + all descendants)
    lvl_id_col = f"L{node_level}_id"
    branch = df[df[lvl_id_col] == str(node_id)].copy()

    # 4) Parent row for headline (rows where deeper levels are empty)
    deeper_id_cols = [f"L{i}_id" for i in range(node_level+1, len(level_cols)+1)]
    if deeper_id_cols:
        parent = branch.copy()
        for c in deeper_id_cols:
            parent = parent[parent[c].isna()]
        if len(parent) == 0:
            # fallback: take the first occurrence at this level
            parent = branch.iloc[[0]].copy()
    else:
        parent = branch.iloc[[0]].copy()

    parent_name = parent[f"L{node_level}_name"].iloc[0] or "Selected Desk"

    # 5) Region (optional mapping overrides heuristics)
    if region_map is not None and {"Desk","Region"}.issubset(region_map.columns):
        # Determine a desk label per row (last non-null name)
        name_cols = [f"L{i}_name" for i in range(1, len(level_cols)+1)]
        branch["Desk"] = branch[name_cols].bfill(axis=1).iloc[:, -1]
        branch = branch.merge(region_map[["Desk","Region"]], on="Desk", how="left")
        branch["Region"] = branch["Region"].fillna(branch["Desk"].map(infer_region))
    else:
        name_cols = [f"L{i}_name" for i in range(1, len(level_cols)+1)]
        branch["Desk"] = branch[name_cols].bfill(axis=1).iloc[:, -1]
        branch["Region"] = branch["Desk"].map(infer_region)

    # 6) Executive snapshot — from the PARENT row only
    exec_total = parent[["VAR95","VAR95_T2","dVaR95","DELTACOVAR95",
                         "VAR99","VAR99_T2","dVaR99","DELTACOVAR99","LIMIT"]].sum()
    snap = pd.DataFrame([{ 
        "Desk": parent_name,
        "VaR95": int(round(exec_total["VAR95"])),
        "ΔVaR95": int(round(exec_total["dVaR95"])),
        "ΔCoVar95": int(round(exec_total["DELTACOVAR95"])),
        "VaR99": int(round(exec_total["VAR99"])),
        "ΔVaR99": int(round(exec_total["dVaR99"])),
        "ΔCoVar99": int(round(exec_total["DELTACOVAR99"])),
        "Limit": int(round(exec_total["LIMIT"])),
        "Util95_%": np.where(exec_total["LIMIT"]!=0, 100*exec_total["VAR95"]/exec_total["LIMIT"], np.nan),
        "Util99_%": np.where(exec_total["LIMIT"]!=0, 100*exec_total["VAR99"]/exec_total["LIMIT"], np.nan),
    }])

    # 7) Immediate child drivers (level = node_level+1) — rank by |ΔVaR95|
    child_lvl = node_level + 1
    if child_lvl in level_cols:
        child_key = f"L{child_lvl}_name"
        child_tab = (branch[branch[child_key].notna()]
                     .groupby(child_key, as_index=False)
                     .agg(VaR95=("VAR95","sum"), dVaR95=("dVaR95","sum"),
                          VaR99=("VAR99","sum"), dVaR99=("dVaR99","sum"),
                          ΔCoVar95=("DELTACOVAR95","sum"), ΔCoVar99=("DELTACOVAR99","sum"),
                          Limit=("LIMIT","sum")))
        child_tab["RankAbs95"] = child_tab["dVaR95"].abs()
        child_tab = child_tab.sort_values(["RankAbs95","dVaR95"], ascending=[False, False]).drop(columns=["RankAbs95"]).head(topn)
        # ints
        for c in ["VaR95","dVaR95","VaR99","dVaR99","ΔCoVar95","ΔCoVar99","Limit"]:
            child_tab[c] = child_tab[c].round().astype(int)
    else:
        child_tab = pd.DataFrame(columns=["Name","VaR95","dVaR95","VaR99","dVaR99","ΔCoVar95","ΔCoVar99","Limit"])

    # 8) Leaf drivers — deepest available name column
    deepest_name_col = None
    for lvl in range(len(level_cols), 0, -1):
        if branch[f"L{lvl}_name"].notna().any():
            deepest_name_col = f"L{lvl}_name"
            break
    leaf = (branch[branch[deepest_name_col].notna()]
            [[deepest_name_col,"Region","VAR95","dVaR95","DELTACOVAR95","VAR99","dVaR99","DELTACOVAR99","LIMIT"]]
            .rename(columns={deepest_name_col:"Leaf"}))
    leaf95 = leaf.copy()
    leaf95["RankAbs"] = leaf95["dVaR95"].abs()
    leaf95 = leaf95.sort_values(["RankAbs","dVaR95"], ascending=[False, False]).drop(columns=["RankAbs"]).head(topn)
    leaf99 = leaf.copy()
    leaf99["RankAbs"] = leaf99["dVaR99"].abs()
    leaf99 = leaf99.sort_values(["RankAbs","dVaR99"], ascending=[False, False]).drop(columns=["RankAbs"]).head(topn)

    # Integer formatting for main numeric cols
    def to_int_cols(df_in: pd.DataFrame, cols: List[str]) -> pd.DataFrame:
        out = df_in.copy()
        for c in cols:
            if c in out.columns:
                out[c] = out[c].round().astype(int)
        return out

    leaf95 = to_int_cols(leaf95, ["VAR95","dVaR95","DELTACOVAR95","VAR99","dVaR99","DELTACOVAR99","LIMIT"])
    leaf99 = to_int_cols(leaf99, ["VAR95","dVaR95","DELTACOVAR95","VAR99","dVaR99","DELTACOVAR99","LIMIT"])

    # 9) Regional roll-up (over branch)
    reg95 = (branch.groupby("Region", as_index=False)
             .agg(VaR95=("VAR95","sum"), dVaR95=("dVaR95","sum"), ΔCoVar95=("DELTACOVAR95","sum")))
    reg99 = (branch.groupby("Region", as_index=False)
             .agg(VaR99=("VAR99","sum"), dVaR99=("dVaR99","sum"), ΔCoVar99=("DELTACOVAR99","sum")))

    # Int-ify region tables
    for df_tmp, cols in [(reg95,["VaR95","dVaR95","ΔCoVar95"]),(reg99,["VaR99","dVaR99","ΔCoVar99"])]:
        for c in cols:
            df_tmp[c] = df_tmp[c].round().astype(int)

    # 10) Neat QA dump of the branch rows
    qa_cols = ["Desk"] + [f"L{i}_name" for i in range(1,len(level_cols)+1)] + [
        "VAR95","VAR95_T2","dVaR95","DELTACOVAR95","VAR99","VAR99_T2","dVaR99","DELTACOVAR99","LIMIT","Util95_%","Util99_%","Region"
    ]
    qa = branch[qa_cols].copy()

    return {
        "parent_name": parent_name,
        "snap": snap,
        "child": child_tab,
        "leaf95": leaf95,
        "leaf99": leaf99,
        "reg95": reg95,
        "reg99": reg99,
        "qa": qa,
    }

# ------------------------------ Writer ---------------------------------

def write_report(out_path: Path, res: Dict[str, pd.DataFrame]):
    with pd.ExcelWriter(out_path, engine="xlsxwriter") as writer:
        wb = writer.book
        intfmt = wb.add_format({"num_format": "#,##0"})
        pctfmt = wb.add_format({"num_format": "0.0%"})
        head = wb.add_format({"bold": True, "bg_color": "#F2F2F2", "border": 1})
        title = wb.add_format({"bold": True, "font_size": 14})

        # Summary
        res["snap"].to_excel(writer, SHEETS["summary"], index=False, startrow=2)
        ws = writer.sheets[SHEETS["summary"]]
        ws.write(0, 0, f"VaR Explain — {res['parent_name']}", title)
        ws.set_column(0, 0, 28)
        ws.set_column(1, 20, 16, intfmt)
        ws.freeze_panes(3, 1)

        # Child drivers
        res["child"].to_excel(writer, SHEETS["drivers_child"], index=False)
        ws = writer.sheets[SHEETS["drivers_child"]]
        ws.set_row(0, None, head)
        ws.autofilter(0, 0, len(res["child"]), res["child"].shape[1]-1)
        ws.set_column(0, 0, 32)
        ws.set_column(1, res["child"].shape[1]-1, 16, intfmt)
        ws.freeze_panes(1, 1)

        # Leaf 95
        res["leaf95"].to_excel(writer, SHEETS["drivers_leaf95"], index=False)
        ws = writer.sheets[SHEETS["drivers_leaf95"]]
        ws.set_row(0, None, head)
        ws.autofilter(0, 0, len(res["leaf95"]), res["leaf95"].shape[1]-1)
        ws.set_column(0, 1, 34)
        ws.set_column(2, res["leaf95"].shape[1]-1, 16, intfmt)
        ws.freeze_panes(1, 1)

        # Leaf 99
        res["leaf99"].to_excel(writer, SHEETS["drivers_leaf99"], index=False)
        ws = writer.sheets[SHEETS["drivers_leaf99"]]
        ws.set_row(0, None, head)
        ws.autofilter(0, 0, len(res["leaf99"]), res["leaf99"].shape[1]-1)
        ws.set_column(0, 1, 34)
        ws.set_column(2, res["leaf99"].shape[1]-1, 16, intfmt)
        ws.freeze_panes(1, 1)

        # Regions
        # Stack 95 and 99 blocks with a gap
        r95 = res["reg95"].copy(); r99 = res["reg99"].copy()
        r95.to_excel(writer, SHEETS["regions"], index=False, startrow=0)
        r99.to_excel(writer, SHEETS["regions"], index=False, startrow=len(r95)+3)
        ws = writer.sheets[SHEETS["regions"]]
        ws.set_row(0, None, head)
        ws.set_row(len(r95)+3, None, head)
        ws.set_column(0, 0, 20)
        ws.set_column(1, 6, 16, intfmt)
        ws.freeze_panes(1, 1)

        # Branch QA
        res["qa"].to_excel(writer, SHEETS["branch"], index=False)
        ws = writer.sheets[SHEETS["branch"]]
        ws.set_row(0, None, head)
        ws.autofilter(0, 0, len(res["qa"]), res["qa"].shape[1]-1)
        ws.set_column(0, 10, 28)
        ws.set_column(11, res["qa"].shape[1]-1, 16, intfmt)
        ws.freeze_panes(1, 1)

# ------------------------------ CLI ------------------------------------

def main():
    ap = argparse.ArgumentParser(description="Hierarchy-aware VaR Explainer for a specific desk node")
    ap.add_argument("--file", required=True, help="Excel export path")
    ap.add_argument("--sheet", default=None, help="Sheet name (default: first)")
    ap.add_argument("--node_id", required=True, help="Node id (e.g., 1373254 for EM Macro)")
    ap.add_argument("--node_level", type=int, default=3, help="Hierarchy level that contains the node id (pl3=3)")
    ap.add_argument("--out", default="VaR_Explain_SelectedDesk.xlsx", help="Output Excel filename")
    ap.add_argument("--topn", type=int, default=12, help="Top N for driver tables")
    ap.add_argument("--map", default=None, help="Optional CSV: Desk,Region for exact region roll-up")
    args = ap.parse_args()

    xls = pd.ExcelFile(args.file)
    sheet = args.sheet or xls.sheet_names[0]
    df = pd.read_excel(xls, sheet_name=sheet, header=0)

    # region mapping (optional)
    region_map = None
    if args.map:
        p = Path(args.map)
        if p.exists():
            tmp = pd.read_csv(p)
            if {"Desk","Region"}.issubset(tmp.columns):
                region_map = tmp

    res = build_branch_frames(df, node_id=str(args.node_id), node_level=int(args.node_level),
                              region_map=region_map, topn=args.topn)

    outp = Path(args.out)
    write_report(outp, res)
    print(f"[OK] Wrote report: {outp.resolve()}")

if __name__ == "__main__":
    main()

























📊 Executive Summary — EM Macro (Node 1373254)

Headline Totals (from parent row EM Macro)

VaR95: £5,841,018

Change vs T-2: +£187,803

Delta CoVar95: –£19,376

VaR99: £9,129,529

Change vs T-2: +£468,224

Delta CoVar99: –£166,651

Limit: £15,000,000

Utilization: ~39% (VaR95) / ~61% (VaR99)

🌍 Regional Drivers

Americas (Node 1373184)

VaR95: ~£2.43m | ΔVaR95: +£134k

Key movers:

EM Rates – NY: ΔVaR95 +£191k (offset by ΔCoVar –£13.6k)

EM FX Forwards & NDF – US: ΔVaR95 +£269k

APAC (Node 1373186)

VaR95: ~£3.99m | ΔVaR95: +£609k

Key movers:

EM Macro – India: ΔVaR95 +£76k

EM Macro – Hong Kong: ΔVaR95 +£56k

EM FX Forwards & NDF – Asia: ΔVaR95 –£50k (negative contribution)

EMEA (Node 1373242, 1373244)

VaR95: ~£1.21m | ΔVaR95: –£619k (largest negative swing)

Drivers:

EM FX Forwards & NDF – Europe: ΔVaR95 –£615k

London Rates sub-desks showing ΔVaR95 –£227k

🔎 Top Drivers of Change

Upside Contributors (ΔVaR95 +):

EM FX Forwards & NDF – US: +£269k

EM Rates – NY: +£191k

EM Macro – India: +£76k

Downside Contributors (ΔVaR95 –):

EM FX Forwards & NDF – Europe: –£615k

London Rates desks (various): –£227k

EM FX Forwards & NDF – Asia: –£50k

📝 Narrative for Management

The headline EM Macro risk is stable within limits — VaR95 at £5.8m vs a £15m limit (39% utilization).

Americas added risk, particularly from US NDFs and NY rates.

APAC showed moderate increases, with India and Hong Kong pushing VaR higher.

EMEA was the key offset, with Europe FX Forwards/NDFs driving a –£615k ΔVaR95.

Overall effect: Net ΔVaR95 +£188k with correlation effects (ΔCoVar –£19k) dampening the increase.
